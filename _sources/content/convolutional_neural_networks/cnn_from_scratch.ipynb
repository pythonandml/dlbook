{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb944c3-c735-41ac-86dc-efb1946e38fe",
   "metadata": {
    "id": "bdb944c3-c735-41ac-86dc-efb1946e38fe"
   },
   "source": [
    "# 3.3. Convolutional Neural Networks from scratch in Python\n",
    "\n",
    "We will be building Convolutional Neural Networks (CNN) model from scratch using Numpy in Python. Please check out the following list of `ingredients` (if you have not already done so), so that you can cook (code) the CNN model from scratch because this is going to be the most general CNN model that you can find anywhere on the net (without using any for loops, except for the epochs part :))!\n",
    "\n",
    "> **Note**: I have already explained (in detail) most of the code sections in my previous chapters (like forward and backward propagation in Convolution layers, forward and backward propagation in Pooling layers, and MLP model for Fully connected layers). I will just put the list for you to go and check them out (so that I can skip the tedious work of explaining them again and concentrate more on the fun part).\n",
    "\n",
    "**Ingredients**\n",
    "\n",
    "* [MLP model from scratch in Python](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/neural_networks_mlp_scratch_best.html)\n",
    "\t\n",
    "* [CNN architecture](https://pythonandml.github.io/dlbook/content/convolutional_neural_networks/cnn_architecture.html) \n",
    "\n",
    "    * [Convolution Layer](https://pythonandml.github.io/dlbook/content/convolutional_neural_networks/convolutional_layers.html)\n",
    "    \n",
    "        * [Forward Propagation Convolution layer (Vectorized)](https://pythonandml.github.io/dlbook/content/convolutional_neural_networks/forward_propagation_convolution.html)\n",
    "        \n",
    "        * [Backward Propagation Convolution layer (Vectorized)](https://pythonandml.github.io/dlbook/content/convolutional_neural_networks/backpropagation_convolution.html)\n",
    "    \n",
    "    * [Pooling Layer](https://pythonandml.github.io/dlbook/content/convolutional_neural_networks/pooling_layers.html)\n",
    "        \n",
    "Now that we have all the ingredients available, we are ready to code the most general `Convolutional Neural Networks (CNN) model` from scratch using Numpy in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c4333-2d2a-4093-ac2c-c8aafc571326",
   "metadata": {
    "id": "1b6c4333-2d2a-4093-ac2c-c8aafc571326"
   },
   "source": [
    "#### Import essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5cb191-e239-4365-8af0-5bea6a6b8708",
   "metadata": {
    "id": "5c5cb191-e239-4365-8af0-5bea6a6b8708",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# numpy for linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib for plotting the loss functions and/or accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# show progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef3240a-94de-472f-8ced-8def31692912",
   "metadata": {
    "id": "0ef3240a-94de-472f-8ced-8def31692912"
   },
   "source": [
    "#### [Activation class](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/activation.html)\n",
    "\n",
    "This class will contain class methods to calculate activation functions and also it will calculate the forward propagation and backpropagation as per the decsription in the chapter [Shortcut to calculate forward pass and backpropagation across layers](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/shortcut_to_calculate_forward_back_propagation.html) (link to previous chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a83947-a79e-425f-87cc-e692cad8862a",
   "metadata": {
    "id": "c3a83947-a79e-425f-87cc-e692cad8862a",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Activation:\n",
    "\n",
    "    def __init__(self, activation_type=None):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        activation_type: type of activation\n",
    "        available options are 'sigmoid', 'linear', 'tanh', 'softmax', 'prelu' and 'relu'\n",
    "        '''\n",
    "        if activation_type is None:\n",
    "            self.activation_type = 'linear'\n",
    "        else:\n",
    "            self.activation_type = activation_type\n",
    "\n",
    "    def linear(self, x):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        x: input matrix of shape (m, d) \n",
    "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
    "        and 'd' is the number of features\n",
    "        '''\n",
    "        return x\n",
    "\n",
    "    def d_linear(self, x):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        x: input matrix of shape (m, d) \n",
    "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
    "        and 'd' is the number of features\n",
    "        '''\n",
    "        return np.ones(x.shape)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        x: input matrix of shape (m, d) \n",
    "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
    "        and 'd' is the number of features\n",
    "        '''\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def d_sigmoid(self, x):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        x: input matrix of shape (m, d) \n",
    "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
    "        and 'd' is the number of features\n",
    "        '''\n",
    "        return self.sigmoid(x) * (1-self.sigmoid(x))\n",
    "     \n",
    "    def tanh(self, x):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        x: input matrix of shape (m, d) \n",
    "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
    "        and 'd' is the number of features\n",
    "        '''\n",
    "        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "    def d_tanh(self, x):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        x: input matrix of shape (m, d) \n",
    "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
    "        and 'd' is the number of features\n",
    "        '''\n",
    "        return 1-(self.tanh(x))**2\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        x: input matrix of shape (m, d) \n",
    "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
    "        and 'd' is the number of features\n",
    "        '''\n",
    "        return x * (x > 0)\n",
    "\n",
    "    def d_ReLU(self, x):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        x: input matrix of shape (m, d) \n",
    "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
    "        and 'd' is the number of features\n",
    "        '''\n",
    "        return (x>0)*np.ones(x.shape)\n",
    "\n",
    "    def PReLU(self, x, alpha=0.2):\n",
    "        '''\n",
    "        Parameters\n",
    "        alpha: slope parameter (𝛼)\n",
    "\n",
    "        x: input matrix of shape (m, d) \n",
    "        where 'm' is the number of samples (or rows)\n",
    "        and 'd' is the number of features (or columns)\n",
    "        '''\n",
    "        return np.where(x > 0, x, alpha*x) \n",
    "\n",
    "    def d_PReLU(self, x, alpha=0.2):\n",
    "        '''\n",
    "        Parameters\n",
    "        alpha: slope parameter (𝛼)\n",
    "\n",
    "        x: input matrix of shape (m, d) \n",
    "        where 'm' is the number of samples (or rows)\n",
    "        and 'd' is the number of features (or columns)\n",
    "        '''\n",
    "        return np.where(x > 0, 1, alpha) \n",
    "\n",
    "    def softmax(self, x):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        x: input matrix of shape (m, d) \n",
    "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
    "        and 'd' is the number of features\n",
    "        '''\n",
    "        z = x - np.max(x, axis=-1, keepdims=True)\n",
    "        numerator = np.exp(z)\n",
    "        denominator = np.sum(numerator, axis=-1, keepdims=True)\n",
    "        softmax = numerator / denominator\n",
    "        return softmax\n",
    "\n",
    "    def d_softmax(self, x):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        x: input matrix of shape (m, d) \n",
    "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
    "        and 'd' is the number of features\n",
    "        '''\n",
    "        if len(x.shape)==1:\n",
    "            x = np.array(x).reshape(1,-1)\n",
    "        else:\n",
    "            x = np.array(x)\n",
    "        m, d = x.shape\n",
    "        a = self.softmax(x)\n",
    "        tensor1 = np.einsum('ij,ik->ijk', a, a)\n",
    "        tensor2 = np.einsum('ij,jk->ijk', a, np.eye(d, d))\n",
    "        return tensor2 - tensor1\n",
    "\n",
    "    def get_activation(self, x):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        x: input matrix of shape (m, d) \n",
    "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
    "        and 'd' is the number of features\n",
    "        '''\n",
    "        if self.activation_type == 'sigmoid':\n",
    "            return self.sigmoid(x)\n",
    "        elif self.activation_type == 'tanh':\n",
    "            return self.tanh(x)\n",
    "        elif self.activation_type == 'relu':\n",
    "            return self.ReLU(x)\n",
    "        elif self.activation_type == 'linear':\n",
    "            return self.linear(x)\n",
    "        elif self.activation_type == 'prelu':\n",
    "            return self.PReLU(x)\n",
    "        elif self.activation_type == 'softmax':\n",
    "            return self.softmax(x)\n",
    "        else:\n",
    "            raise ValueError(\"Valid Activations are only 'sigmoid', 'linear', 'tanh' 'softmax', 'prelu' and 'relu'\")\n",
    "\n",
    "    def get_d_activation(self, x):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        x: input matrix of shape (m, d) \n",
    "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
    "        and 'd' is the number of features\n",
    "        '''\n",
    "        if self.activation_type == 'sigmoid':\n",
    "            return self.d_sigmoid(x)\n",
    "        elif self.activation_type == 'tanh':\n",
    "            return self.d_tanh(x)\n",
    "        elif self.activation_type == 'relu':\n",
    "            return self.d_ReLU(x)\n",
    "        elif self.activation_type == 'linear':\n",
    "            return self.d_linear(x)\n",
    "        elif self.activation_type == 'prelu':\n",
    "            return self.d_PReLU(x)\n",
    "        elif self.activation_type == 'softmax':\n",
    "            return self.d_softmax(x)\n",
    "        else:\n",
    "            raise ValueError(\"Valid Activations are only 'sigmoid', 'linear', 'tanh', 'softmax', 'prelu' and 'relu'\")\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        z = self.get_activation(X)\n",
    "        return z\n",
    "    \n",
    "    def backpropagation(self, dz):\n",
    "        f_prime = self.get_d_activation(self.X)\n",
    "        if self.activation_type=='softmax':\n",
    "            # because derivative of softmax is a tensor\n",
    "            dx = np.einsum('ijk,ik->ij', f_prime, dz)\n",
    "        else:\n",
    "            dx = dz * f_prime\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774d77ff-8791-4905-876a-2d7d8ea92223",
   "metadata": {
    "id": "774d77ff-8791-4905-876a-2d7d8ea92223"
   },
   "source": [
    "#### [Cost function](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/cost_functions.html) \n",
    "\n",
    "Follow the lecture to develop the cost function class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7020c13-65bf-489f-a81a-ed31ab7c768a",
   "metadata": {
    "id": "a7020c13-65bf-489f-a81a-ed31ab7c768a",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Cost:\n",
    "\n",
    "    def __init__(self, cost_type='mse'):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        cost_type: type of cost function\n",
    "        available options are 'mse', and 'cross-entropy'\n",
    "        '''\n",
    "        self.cost_type = cost_type\n",
    "\n",
    "    def mse(self, a, y):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        a: Predicted output array of shape (m, d)\n",
    "        y: Actual output array of shape (m, d)\n",
    "        '''\n",
    "        return (1/2)*np.sum((np.linalg.norm(a-y, axis=1))**2)\n",
    "\n",
    "    def d_mse(self, a, y):\n",
    "        '''\n",
    "        represents dJ/da\n",
    "\n",
    "        Parameters\n",
    "        \n",
    "        a: Predicted output array of shape (m, d)\n",
    "        y: Actual output array of shape (m, d)\n",
    "        '''\n",
    "        return a - y\n",
    "\n",
    "    def cross_entropy(self, a, y, epsilon=1e-12):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        a: Predicted output array of shape (m, d)\n",
    "        y: Actual output array of shape (m, d)\n",
    "        '''\n",
    "        a = np.clip(a, epsilon, 1. - epsilon)\n",
    "        return -np.sum(y*np.log(a))\n",
    "\n",
    "    def d_cross_entropy(self, a, y, epsilon=1e-12):\n",
    "        '''\n",
    "        represents dJ/da\n",
    "\n",
    "        Parameters\n",
    "        \n",
    "        a: Predicted output array of shape (m, d)\n",
    "        y: Actual output array of shape (m, d)\n",
    "        '''\n",
    "        a = np.clip(a, epsilon, 1. - epsilon)\n",
    "        return -y/a\n",
    "\n",
    "    def get_cost(self, a, y):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        a: Predicted output array of shape (m, d)\n",
    "        y: Actual output array of shape (m, d)\n",
    "        '''\n",
    "        if self.cost_type == 'mse':\n",
    "            return self.mse(a, y)\n",
    "        elif self.cost_type == 'cross-entropy':\n",
    "            return self.cross_entropy(a, y)\n",
    "        else:\n",
    "            raise ValueError(\"Valid cost functions are only 'mse', and 'cross-entropy'\")\n",
    "\n",
    "    def get_d_cost(self, a, y):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        a: Predicted output array of shape (m, d)\n",
    "        y: Actual output array of shape (m, d)\n",
    "        '''\n",
    "        if self.cost_type == 'mse':\n",
    "            return self.d_mse(a, y)\n",
    "        elif self.cost_type == 'cross-entropy':\n",
    "            return self.d_cross_entropy(a, y)\n",
    "        else:\n",
    "            raise ValueError(\"Valid cost functions are only 'mse', and 'cross-entropy'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84e00ad-6a32-4b22-a7b1-8faaf747e3a4",
   "metadata": {
    "id": "b84e00ad-6a32-4b22-a7b1-8faaf747e3a4"
   },
   "source": [
    "#### [Optimizers](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/gradient_descent.html)\n",
    "\n",
    "This class contains different optimizers (such as RMSProp, Adam, etc) used for updating the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4897adb5-b894-42b2-aea0-51111997b091",
   "metadata": {
    "id": "4897adb5-b894-42b2-aea0-51111997b091",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "\n",
    "    def __init__(self, optimizer_type=None, shape_W=None, shape_b=None,\n",
    "                 momentum1=0.9, momentum2=0.999, epsilon=1e-8):\n",
    "        '''\n",
    "        Parameters\n",
    "\n",
    "        momentum1: float hyperparameter >= 0 that accelerates gradient descent in the relevant \n",
    "                   direction and dampens oscillations. Defaults to 0, i.e., vanilla gradient descent.\n",
    "                   Also used in RMSProp\n",
    "        momentum2: used in Adam only\n",
    "        optimizer_type: type of optimizer\n",
    "                        available options are 'gd', 'sgd' (This also includes momentum), 'adam', and 'rmsprop'\n",
    "        shape_W: Shape of the weight matrix W/ Kernel K\n",
    "        shape_b: Shape of the bias matrix b\n",
    "        epsilon: parameter used in RMSProp and Adam to avoid division by zero error\n",
    "        '''\n",
    "\n",
    "        if optimizer_type is None:\n",
    "            self.optimizer_type = 'adam'\n",
    "        else:\n",
    "            self.optimizer_type = optimizer_type\n",
    "\n",
    "        self.momentum1 = momentum1\n",
    "        self.momentum2 = momentum2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.vdW = np.zeros(shape_W)\n",
    "        self.vdb = np.zeros(shape_b)\n",
    "\n",
    "        self.SdW = np.zeros(shape_W)\n",
    "        self.Sdb = np.zeros(shape_b)\n",
    "\n",
    "    def GD(self, dW, db, k):\n",
    "        '''\n",
    "        dW: gradient of Weight W for iteration k\n",
    "        db: gradient of bias b for iteration k\n",
    "        k: iteration number\n",
    "        '''\n",
    "        return dW, db\n",
    " \n",
    "    def SGD(self, dW, db, k):\n",
    "        '''\n",
    "        dW: gradient of Weight W for iteration k\n",
    "        db: gradient of bias b for iteration k\n",
    "        k: iteration number\n",
    "        '''\n",
    "        self.vdW = self.momentum1*self.vdW + (1-self.momentum1)*dW\n",
    "        self.vdb = self.momentum1*self.vdb + (1-self.momentum1)*db\n",
    "\n",
    "        return self.vdW, self.vdb\n",
    "\n",
    "    def RMSProp(self, dW, db, k):\n",
    "        '''\n",
    "        dW: gradient of Weight W for iteration k\n",
    "        db: gradient of bias b for iteration k\n",
    "        k: iteration number\n",
    "        '''\n",
    "        self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)\n",
    "        self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)\n",
    "\n",
    "        den_W = np.sqrt(self.SdW) + self.epsilon\n",
    "        den_b = np.sqrt(self.Sdb) + self.epsilon\n",
    "\n",
    "        return dW/den_W, db/den_b\n",
    " \n",
    "    def Adam(self, dW, db, k):\n",
    "        '''\n",
    "        dW: gradient of Weight W for iteration k\n",
    "        db: gradient of bias b for iteration k\n",
    "        k: iteration number\n",
    "        '''\n",
    "        # momentum\n",
    "        self.vdW = self.momentum1*self.vdW + (1-self.momentum1)*dW\n",
    "        self.vdb = self.momentum1*self.vdb + (1-self.momentum1)*db\n",
    "\n",
    "        # rmsprop\n",
    "        self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)\n",
    "        self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)\n",
    "\n",
    "        # correction\n",
    "        if k>1:\n",
    "            vdW_h = self.vdW / (1-(self.momentum1**k))\n",
    "            vdb_h = self.vdb / (1-(self.momentum1**k))\n",
    "            SdW_h = self.SdW / (1-(self.momentum2**k))\n",
    "            Sdb_h = self.Sdb / (1-(self.momentum2**k))\n",
    "        else:\n",
    "            vdW_h = self.vdW \n",
    "            vdb_h = self.vdb\n",
    "            SdW_h = self.SdW\n",
    "            Sdb_h = self.Sdb\n",
    "\n",
    "        den_W = np.sqrt(SdW_h) + self.epsilon\n",
    "        den_b = np.sqrt(Sdb_h) + self.epsilon\n",
    "\n",
    "        return vdW_h/den_W, vdb_h/den_b\n",
    "\n",
    "    def get_optimization(self, dW, db, k):\n",
    "        if self.optimizer_type == 'gd':\n",
    "            return self.GD(dW, db, k)\n",
    "        if self.optimizer_type == 'sgd':\n",
    "            return self.SGD(dW, db, k)\n",
    "        if self.optimizer_type == 'rmsprop':\n",
    "            return self.RMSProp(dW, db, k)\n",
    "        if self.optimizer_type == 'adam':\n",
    "            return self.Adam(dW, db, k)\n",
    "        else:\n",
    "            raise ValueError(\"Valid optimizer options are only 'gd', 'sgd', 'rmsprop', and 'adam'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3182ab77-9d25-4371-8bb9-bc1a35e52eee",
   "metadata": {
    "id": "3182ab77-9d25-4371-8bb9-bc1a35e52eee"
   },
   "source": [
    "#### [Learning Rate decay](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/gradient_descent.html#learning-rate-decay)\n",
    "\n",
    "This class contains different methods to implement the learning rate decay scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd434f94-2e43-4b8b-b475-8d7d27b657a2",
   "metadata": {
    "id": "bd434f94-2e43-4b8b-b475-8d7d27b657a2",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class LearningRateDecay:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def constant(self, t, lr_0):\n",
    "        '''\n",
    "        t: iteration\n",
    "        lr_0: initial learning rate\n",
    "        '''\n",
    "        return lr_0\n",
    "\n",
    "    def time_decay(self, t, lr_0, k):\n",
    "        '''\n",
    "        lr_0: initial learning rate\n",
    "        k: Decay rate\n",
    "        t: iteration number\n",
    "        '''\n",
    "        lr = lr_0 /(1+(k*t))\n",
    "        return lr\n",
    "\n",
    "    def step_decay(self, t, lr_0, F, D):\n",
    "        '''\n",
    "        lr_0: initial learning rate\n",
    "        F: factor value controlling the rate in which the learning date drops\n",
    "        D: “Drop every” iteration\n",
    "        t: current iteration\n",
    "        '''\n",
    "        mult = F**np.floor((1+t)/D)\n",
    "        lr = lr_0 * mult\n",
    "        return lr\n",
    "\n",
    "    def exponential_decay(self, t, lr_0, k):\n",
    "        '''\n",
    "        lr_0: initial learning rate\n",
    "        k: Exponential Decay rate\n",
    "        t: iteration number\n",
    "        '''\n",
    "        lr = lr_0 * np.exp(-k*t)\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7416475b-b38c-4bcf-98f6-11f760aa25bc",
   "metadata": {
    "id": "7416475b-b38c-4bcf-98f6-11f760aa25bc"
   },
   "source": [
    "#### [Utility function](https://pythonandml.github.io/dlbook/content/preliminaries/data_preprocessing.html)\n",
    "\n",
    "This class contains several utility functions such as one-hot vector, label encoder, normalization, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf2357-cdbf-4432-9fdd-340d1ca8de12",
   "metadata": {
    "id": "0daf2357-cdbf-4432-9fdd-340d1ca8de12",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Utility:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def label_encoding(self, Y):\n",
    "        '''\n",
    "        Parameters:\n",
    "        Y: (m,d) shape matrix with categorical data\n",
    "        Return\n",
    "        result: label encoded data of 𝑌\n",
    "        idx_list: list of the dictionaries containing the unique values \n",
    "                  of the columns and their mapping to the integer.\n",
    "        '''\n",
    "        idx_list = []\n",
    "        result = []\n",
    "        for col in range(Y.shape[1]):\n",
    "            indexes = {val: idx for idx, val in enumerate(np.unique(Y[:, col]))}\n",
    "            result.append([indexes[s] for s in Y[:, col]])\n",
    "            idx_list.append(indexes)\n",
    "        return np.array(result).T, idx_list\n",
    "\n",
    "    def onehot(self, X):\n",
    "        '''\n",
    "        Parameters:\n",
    "        X: 1D array of labels of length \"m\"\n",
    "        Return\n",
    "        X_onehot: (m,d) one hot encoded matrix (one-hot of X) \n",
    "                  (where d is the number of unique values in X)\n",
    "        indexes: dictionary containing the unique values of X and their mapping to the integer column\n",
    "        '''\n",
    "        indexes = {val: idx for idx, val in enumerate(np.unique(X))}\n",
    "        y = np.array([indexes[s] for s in X])\n",
    "        X_onehot = np.zeros((y.size, len(indexes)))\n",
    "        X_onehot[np.arange(y.size), y] = 1\n",
    "        return X_onehot, indexes\n",
    "\n",
    "    def minmax(self, X, min_X=None, max_X=None):\n",
    "        if min_X is None:\n",
    "            min_X = np.min(X, axis=0)\n",
    "        if max_X is None:\n",
    "            max_X = np.max(X, axis=0)\n",
    "        Z = (X - min_X) / (max_X - min_X)\n",
    "        return Z, min_X, max_X\n",
    "\n",
    "    def standardize(self, X, mu=None, std=None):\n",
    "        if mu is None:\n",
    "            mu = np.mean(X, axis=0)\n",
    "        if std is None:\n",
    "            std = np.std(X, axis=0)\n",
    "        Z = (X - mu) / std\n",
    "        return Z, mu, std\n",
    "\n",
    "    def inv_standardize(self, Z, mu, std):\n",
    "        X = Z*std + mu\n",
    "        return X\n",
    "\n",
    "    def train_test_split(self, X, y, test_ratio=0.2, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        train_ratio = 1-test_ratio\n",
    "        indices = np.random.permutation(X.shape[0])\n",
    "        train_idx, test_idx = indices[:int(train_ratio*len(X))], indices[int(train_ratio*len(X)):]\n",
    "        X_train, X_test = X[train_idx,:], X[test_idx,:]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e65c14-d669-45d4-93a7-411eb5aa73a9",
   "metadata": {
    "id": "f7e65c14-d669-45d4-93a7-411eb5aa73a9"
   },
   "source": [
    "#### [Weights initializer class](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/terminologies_part_1.html#parameter-s-initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e655844-7ced-4c5d-b2d3-78879d84a555",
   "metadata": {
    "id": "3e655844-7ced-4c5d-b2d3-78879d84a555",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Weights_initializer:\n",
    "\n",
    "    def __init__(self, shape, initializer_type=None, seed=None):\n",
    "        '''\n",
    "        Parameters\n",
    "        shape: Shape of the weight matrix\n",
    "\n",
    "        initializer_type: type of weight initializer\n",
    "        available options are 'zeros', 'ones', 'random_normal', 'random_uniform', \n",
    "        'he_normal', 'xavier_normal' and 'glorot_normal'\n",
    "        '''\n",
    "        self.shape = shape\n",
    "        if initializer_type is None:\n",
    "            self.initializer_type = \"he_normal\"\n",
    "        else:\n",
    "            self.initializer_type = initializer_type\n",
    "        self.seed = seed\n",
    "        \n",
    "    def zeros_initializer(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        return np.zeros(self.shape)\n",
    "\n",
    "    def ones_initializer(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        return np.ones(self.shape)\n",
    "\n",
    "    def random_normal_initializer(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        return np.random.normal(size=self.shape)\n",
    "\n",
    "    def random_uniform_initializer(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        return np.random.uniform(size=self.shape)\n",
    "\n",
    "    def he_initializer(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        try:\n",
    "            F, Kc, Kh, Kw = self.shape\n",
    "        except:\n",
    "            Kh, Kw = self.shape\n",
    "        return np.random.randn(*self.shape) * np.sqrt(2/Kh)\n",
    "\n",
    "    def xavier_initializer(self):\n",
    "        '''\n",
    "        shape: Shape of the Kernel matrix.\n",
    "        '''\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        try:\n",
    "            F, Kc, Kh, Kw = self.shape\n",
    "        except:\n",
    "            Kh, Kw = self.shape\n",
    "        return np.random.randn(*self.shape) * np.sqrt(1/Kh)\n",
    "\n",
    "    def glorot_initializer(self):\n",
    "        '''\n",
    "        shape: Shape of the weight matrix.\n",
    "        '''\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        try:\n",
    "            F, Kc, Kh, Kw = self.shape\n",
    "        except:\n",
    "            Kh, Kw = self.shape\n",
    "        return np.random.randn(*self.shape) * np.sqrt(2/(Kh+Kw))\n",
    "\n",
    "    def get_initializer(self):\n",
    "        if self.initializer_type == 'zeros':\n",
    "            return self.zeros_initializer()\n",
    "        elif self.initializer_type == 'ones':\n",
    "            return self.ones_initializer()\n",
    "        elif self.initializer_type == 'random_normal':\n",
    "            return self.random_normal_initializer()\n",
    "        elif self.initializer_type == 'random_uniform':\n",
    "            return self.random_uniform_initializer()\n",
    "        elif self.initializer_type == 'he_normal':\n",
    "            return self.he_initializer()\n",
    "        elif self.initializer_type == 'xavier_normal':\n",
    "            return self.xavier_initializer()\n",
    "        elif self.initializer_type == 'glorot_normal':\n",
    "            return self.glorot_initializer()\n",
    "        else:\n",
    "            raise ValueError(\"Valid initializer options are 'zeros', 'ones', 'random_normal', 'random_uniform', 'he_normal', 'xavier_normal', and 'glorot_normal'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cc5276-8886-41fc-b845-f6eeb896d93b",
   "metadata": {
    "id": "37cc5276-8886-41fc-b845-f6eeb896d93b"
   },
   "source": [
    "#### Dense class\n",
    "\n",
    "Dense class implements the operation: \n",
    "\n",
    "$$\n",
    "z = XW + b^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "a = f(z) \n",
    "$$\n",
    "\n",
    "where activation $f(.)$ is used if specified, else we do not use it. $W$ is a weights matrix created by the Dense layer based on [type of initialization](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/terminologies_part_1.html#parameter-s-initialization) (link to previous chapter) provided, and $b$ is a bias vector created by the layer (only applicable if use_bias is True). These are all attributes of Dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93298d4-837b-4288-b473-8491bb70fce6",
   "metadata": {
    "id": "b93298d4-837b-4288-b473-8491bb70fce6",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "\n",
    "    def __init__(self, neurons, activation_type=None, use_bias=True, \n",
    "                 weight_initializer_type=None, weight_regularizer=None, seed=None, input_dim=None):\n",
    "\n",
    "        '''\n",
    "        Parameters:\n",
    "\n",
    "        neurons: Positive integer (number of neurons), dimensionality of the output\n",
    "\n",
    "        activation_type: type of activation\n",
    "                         available options are 'sigmoid', 'linear', 'tanh', 'softmax', 'prelu' and 'relu'\n",
    "                         If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "\n",
    "        use_bias: Boolean, whether the layer uses a bias vector.\n",
    "        \n",
    "        weight_initializer_type: Initializer for the kernel weights matrix.\n",
    "        \n",
    "        weight_regularizer: Tuple, Regularizer function applied to the weights matrix ('L2', 0.01) or ('L1', 2)\n",
    "\n",
    "        seed: To generate reproducable results\n",
    "\n",
    "        input_dim: integer showing number of neurons in input layer \n",
    "        '''\n",
    "        self.neurons = neurons\n",
    "        self.activation = Activation(activation_type=activation_type)\n",
    "        self.use_bias = use_bias\n",
    "        self.weight_initializer_type = weight_initializer_type # none is handled\n",
    "        if weight_regularizer is None:\n",
    "            self.weight_regularizer = ('L2', 0)\n",
    "        else:\n",
    "            self.weight_regularizer = weight_regularizer\n",
    "        self.seed = seed\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "\n",
    "    def initialize_parameters(self, hl, optimizer_type):\n",
    "        '''\n",
    "        hl: Number of neurons in layer l-1\n",
    "        '''\n",
    "        shape_W = (hl, self.neurons)\n",
    "        shape_b = (self.neurons, 1)\n",
    "        initializer = Weights_initializer(shape=shape_W,\n",
    "                                          initializer_type=self.weight_initializer_type,\n",
    "                                          seed=self.seed)\n",
    "        self.W = initializer.get_initializer()\n",
    "        self.b = np.zeros(shape_b)\n",
    "        \n",
    "        self.optimizer = Optimizer(optimizer_type=optimizer_type, shape_W=shape_W, shape_b=shape_b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        r = X @ self.W\n",
    "        self.z = r + self.b.T\n",
    "        a = self.activation.forward(self.z)\n",
    "        return a\n",
    "    \n",
    "    def backpropagation(self, da):\n",
    "        dz = self.activation.backpropagation(da)\n",
    "        dr = dz.copy()\n",
    "        self.db = np.sum(dz, axis=0).reshape(-1,1)\n",
    "        self.dW = (self.X.T) @ dr\n",
    "        dX = dr @ (self.W.T)\n",
    "        return dX\n",
    "\n",
    "    def update(self, lr, m, k):\n",
    "        '''\n",
    "        Parameters:\n",
    "\n",
    "        lr: learning rate\n",
    "        m: batch_size (sumber of samples in batch)\n",
    "        k: iteration_number\n",
    "        '''\n",
    "        dW, db = self.optimizer.get_optimization(self.dW, self.db, k)\n",
    "\n",
    "        if self.weight_regularizer[0].lower()=='l2':\n",
    "            dW += self.weight_regularizer[1] * self.W\n",
    "        elif self.weight_regularizer[0].lower()=='l1':\n",
    "            dW += self.weight_regularizer[1] * np.sign(self.W)\n",
    "        \n",
    "        self.W -= dW*(lr/m)\n",
    "        if self.use_bias:\n",
    "            self.b -= db*(lr/m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3cc76-c419-492f-a2a2-cbf98008f375",
   "metadata": {
    "id": "dbf3cc76-c419-492f-a2a2-cbf98008f375"
   },
   "source": [
    "#### [Dropout class](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/dropout.html)\n",
    "\n",
    "This class will perform forward and backpropagation for a Dropout layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786fad9-4395-4963-9705-122a049f02d0",
   "metadata": {
    "id": "8786fad9-4395-4963-9705-122a049f02d0",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "\n",
    "    def __init__(self, p):\n",
    "        '''\n",
    "        Parameters\n",
    "\n",
    "        p: Dropout probability\n",
    "        '''\n",
    "        self.p = p\n",
    "        if self.p == 0:\n",
    "            self.p += 1e-6\n",
    "        if self.p == 1:\n",
    "            self.p -= 1e-6\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.mask = (np.random.rand(*X.shape) < self.p) / self.p \n",
    "        Z = X * self.mask\n",
    "        return Z\n",
    "    \n",
    "    def backpropagation(self, dZ):\n",
    "        dX = dZ * self.mask\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d7c66-003d-40c5-a286-3fedac6442db",
   "metadata": {
    "id": "e61d7c66-003d-40c5-a286-3fedac6442db"
   },
   "source": [
    "#### [Batch Normalization class](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/batch_normalization.html)\n",
    "\n",
    "This class will perform forward and backpropagation for Batch Normalization layer. Right now it is only prepared for 1D data (that is it can be used only in Fully Connected or MLP layers and not in Convolution or Maxpool).\n",
    "\n",
    "> **Note:** We will initialise $\\gamma$ as ones and $\\beta$ as zeroes so that the output of the linear batch-norm transformation initially follows the standard zero-mean unit-variance normal distribution. This provides a normalised starting point, for which the model can update the $\\gamma$ and $\\beta$ to scale and shift the distribution(s) of each input accordingly (for the current layer). \n",
    "\n",
    "**Forward pass**\n",
    "\n",
    "`eps` represents: $\\epsilon$\n",
    "\n",
    "`mu` represents: $\\mu$\n",
    "\n",
    "`var` represents: $\\sigma^2$\n",
    "\n",
    "`zmu` represents: $\\bar{z_l}$\n",
    "\n",
    "`ivar` represents: $\\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}}$\n",
    "\n",
    "`zhat` represents: $\\hat{z_l}$\n",
    "\n",
    "`q` represents: $q_l$\n",
    "\n",
    "**Backpropagation**\n",
    "\n",
    "This `dq` variable below represents $\\frac{\\partial J}{\\partial q_l}$\n",
    "\n",
    "`dgamma` represents: $\\frac{\\partial J}{\\partial \\gamma}$\n",
    "\n",
    "`dbeta` represents: $\\frac{\\partial J}{\\partial \\beta}$\n",
    "\n",
    "`dzhat` represents: $\\frac{\\partial J}{\\partial \\hat{z_l}}$\n",
    "\n",
    "`dvar` represents: $\\frac{\\partial J}{\\partial \\sigma^2}$\n",
    "\n",
    "`dmu` represents: $\\frac{\\partial J}{\\partial \\mu}$\n",
    "\n",
    "`dz` represents: $\\frac{\\partial J}{\\partial z_l}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c01495-ae8a-4997-bb17-fc6bcc199f56",
   "metadata": {
    "id": "58c01495-ae8a-4997-bb17-fc6bcc199f56",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "\n",
    "    def __init__(self, momentum=0.9, epsilon=1e-6):\n",
    "        '''\n",
    "        Parameters\n",
    "\n",
    "        momentum: Momentum for the moving average\n",
    "        epsilon: 𝜖, Small float added to variance to avoid dividing by zero\n",
    "        '''\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    def initialize_parameters(self, d):\n",
    "        '''\n",
    "        d: Shape of input to BN layer\n",
    "        '''\n",
    "        self.gamma = np.ones((d))\n",
    "        self.beta = np.zeros((d))\n",
    "        self.running_mean = np.zeros((d))\n",
    "        self.running_var = np.zeros((d))\n",
    "    \n",
    "    def forward(self, z, mode='train'):\n",
    "        '''\n",
    "        z: Input to BN layer\n",
    "        mode: forward pass used for train or test\n",
    "        ''' \n",
    "        if mode=='train':\n",
    "            self.m, self.d = z.shape\n",
    "            self.mu = np.mean(z, axis = 0) # 𝜇\n",
    "            self.var = np.var(z, axis=0) # 𝜎^2\n",
    "            self.zmu = z - self.mu # z - 𝜇\n",
    "            self.ivar = 1 / np.sqrt(self.var + self.epsilon) # 𝜎𝑖𝑛𝑣\n",
    "            self.zhat = self.zmu * self.ivar \n",
    "            q = self.gamma*self.zhat + self.beta # ql\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mu\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
    "        elif mode=='test':\n",
    "            q = (z - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            q = self.gamma*q + self.beta\n",
    "        else:\n",
    "            raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
    "        return q\n",
    "\n",
    "    def backpropagation(self, dq):\n",
    "        self.dgamma = np.sum(dq * self.zhat, axis=0)\n",
    "        self.dbeta = np.sum(dq, axis=0)\n",
    "        dzhat = dq * self.gamma\n",
    "        dvar = np.sum(dzhat * self.zmu * (-.5) * (self.ivar**3), axis=0)\n",
    "        dmu = np.sum(dzhat * (-self.ivar), axis=0)\n",
    "        dz = dzhat * self.ivar + dvar * (2/self.m) * self.zmu + (1/self.m)*dmu\n",
    "        return dz\n",
    "\n",
    "    def update(self, lr, m, k):\n",
    "        '''\n",
    "        Parameters:\n",
    "\n",
    "        lr: learning rate\n",
    "        m: batch_size (sumber of samples in batch)\n",
    "        k: iteration_number\n",
    "        '''\n",
    "        self.gamma -= self.dgamma*(lr/m)\n",
    "        self.beta -= self.dbeta*(lr/m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18d3204-0701-437c-bb4e-2f043754ac6d",
   "metadata": {
    "id": "d18d3204-0701-437c-bb4e-2f043754ac6d"
   },
   "source": [
    "#### [Padding2D class](https://pythonandml.github.io/dlbook/content/convolutional_neural_networks/convolutional_layers.html)\n",
    "\n",
    "This class will perform padding on a batch of 2D image with channels. We will be considering padding as a separate layer only which contains its own forward and backpropagation operations.\n",
    "\n",
    "We need a small function such that we can extract the original input errors from the padding ones. This you can consider as **backpropagation through padding operation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36dc1c-a91a-4d49-b48b-c3341b766a59",
   "metadata": {
    "id": "2b36dc1c-a91a-4d49-b48b-c3341b766a59",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Padding2D:\n",
    "\n",
    "    def __init__(self, p='valid'):\n",
    "        '''\n",
    "        Parameters:\n",
    "\n",
    "        p: padding type\n",
    "        Allowed types are only 'same', 'valid', an integer or a tuple of length 2.\n",
    "        '''\n",
    "        self.p = p\n",
    "        \n",
    "    def get_dimensions(self, input_shape, kernel_size, s=(1,1)):\n",
    "        '''\n",
    "        Utility function to help get the dimension of the output after padding\n",
    "        '''\n",
    "        if len(input_shape)==4:\n",
    "            m, Nc, Nh, Nw = input_shape\n",
    "        elif len(input_shape)==3:\n",
    "            Nc, Nh, Nw = input_shape\n",
    "        \n",
    "        Kh, Kw = kernel_size\n",
    "        sh, sw = s\n",
    "        p = self.p\n",
    "        \n",
    "        if type(p)==int:\n",
    "            pt, pb = p, p\n",
    "            pl, pr = p, p\n",
    "\n",
    "        if type(p)==tuple:\n",
    "            ph, pw = p\n",
    "            pt, pb = ph//2, (ph+1)//2\n",
    "            pl, pr = pw//2, (pw+1)//2\n",
    "\n",
    "        elif p=='valid':\n",
    "            pt, pb = 0, 0\n",
    "            pl, pr = 0, 0\n",
    "\n",
    "        elif p=='same':\n",
    "            # calculating how much padding is required in all 4 directions \n",
    "            # (top, bottom, left and right)\n",
    "            ph = (sh-1)*Nh + Kh - sh\n",
    "            pw = (sw-1)*Nw + Kw - sw\n",
    "\n",
    "            pt, pb = ph//2, (ph+1)//2\n",
    "            pl, pr = pw//2, (pw+1)//2\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Incorrect padding type. Allowed types are only 'same', 'valid', an integer or a tuple of length 2.\")\n",
    "            \n",
    "        if len(input_shape)==4:\n",
    "            output_shape = (m, Nc, Nh+pt+pb, Nw+pl+pr)\n",
    "        elif len(input_shape)==3:\n",
    "            output_shape = (Nc, Nh+pt+pb, Nw+pl+pr)\n",
    "        \n",
    "        return output_shape, (pt, pb, pl, pr)\n",
    "        \n",
    "    def forward(self, X, kernel_size, s=(1,1)):\n",
    "        '''\n",
    "        Parameters:\n",
    "        \n",
    "        X: input of shape (m, Nc, Nh, Nw)\n",
    "        \n",
    "        s: strides along height and width (sh, sw)\n",
    "        \n",
    "        kernel_size: kernel size as specified in Conv2D layer\n",
    "        \n",
    "        Returns: \n",
    "        \n",
    "        Xp: padded X\n",
    "        '''\n",
    "        self.input_shape = X.shape\n",
    "        m, Nc, Nh, Nw = self.input_shape\n",
    "        \n",
    "        self.output_shape, (self.pt, self.pb, self.pl, self.pr) = self.get_dimensions(self.input_shape, \n",
    "                                                                                      kernel_size, s=s)\n",
    "        \n",
    "        zeros_r = np.zeros((m, Nc, Nh, self.pr))\n",
    "        zeros_l = np.zeros((m, Nc, Nh, self.pl))\n",
    "        zeros_t = np.zeros((m, Nc, self.pt, Nw + self.pl + self.pr))\n",
    "        zeros_b = np.zeros((m, Nc, self.pb, Nw + self.pl + self.pr))\n",
    "\n",
    "        Xp = np.concatenate((X, zeros_r), axis=3)\n",
    "        Xp = np.concatenate((zeros_l, Xp), axis=3)\n",
    "        Xp = np.concatenate((zeros_t, Xp), axis=2)\n",
    "        Xp = np.concatenate((Xp, zeros_b), axis=2)\n",
    "        \n",
    "        return Xp\n",
    "    \n",
    "    def backpropagation(self, dXp):\n",
    "        '''\n",
    "        Parameters: \n",
    "        \n",
    "        dXp: Backprop Error of padded X (Xp)\n",
    "        \n",
    "        Return:\n",
    "        \n",
    "        dX: Backprop Error of X\n",
    "        '''\n",
    "        m, Nc, Nh, Nw = self.input_shape\n",
    "        dX = dXp[:, :, self.pt:self.pt+Nh, self.pl:self.pl+Nw]\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec72fc0-3d72-4596-bf98-8a3e5fc61155",
   "metadata": {
    "id": "fec72fc0-3d72-4596-bf98-8a3e5fc61155"
   },
   "source": [
    "#### [Convolution2D class](https://pythonandml.github.io/dlbook/content/convolutional_neural_networks/convolutional_layers.html)\n",
    "\n",
    "This class will perform forward and backpropagation for a Convolution layer on a batch of 2D image with channels.\n",
    "\n",
    "We have an Input with batch of images (with channels) and multiple filters.\n",
    "\n",
    "Shape of Image will be $(m, N_c, N_h, N_w)$.\n",
    "\n",
    "The shape of Kernel will be $(F, K_c, K_h, K_w)$ where $F$ is the total number of filters.\n",
    "\n",
    "![](images/input_batch_channels_and_filters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fce354-64bb-49fd-af7c-a40e45a81d53",
   "metadata": {
    "id": "73fce354-64bb-49fd-af7c-a40e45a81d53"
   },
   "source": [
    "![](images/convolution_layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c5f5cd-eea7-4208-b091-a2a29668fd80",
   "metadata": {
    "id": "b5c5f5cd-eea7-4208-b091-a2a29668fd80",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "\n",
    "    def __init__(self, filters, kernel_size, s=(1, 1), p='valid',\n",
    "                 activation_type=None, use_bias=True, weight_initializer_type=None, \n",
    "                 kernel_regularizer=None, seed=None, input_shape=None):\n",
    "\n",
    "        '''\n",
    "        Parameters:\n",
    "\n",
    "        filters: Integer, the number of output filters in the convolution (F).\n",
    "        \n",
    "        kernel_size: An integer or tuple/list of 2 integers, \n",
    "                     specifying the height and width of the 2D convolution window.\n",
    "                     \n",
    "        s: strides along height and width (sh, sw)\n",
    "        \n",
    "        p: padding type\n",
    "           Allowed types are only 'same', 'valid', an integer or a tuple of length 2.\n",
    "\n",
    "        activation_type: type of activation\n",
    "                         available options are 'sigmoid', 'linear', 'tanh', 'softmax', 'prelu' and 'relu'\n",
    "                         If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "\n",
    "        use_bias: Boolean, whether the layer uses a bias vector.\n",
    "        \n",
    "        weight_initializer_type: Initializer for the kernel weights matrix.\n",
    "        \n",
    "        kernel_regularizer: Tuple, Regularizer function applied to the kernel matrix ('L2', 0.01) or ('L1', 2)\n",
    "\n",
    "        seed: To generate reproducable results\n",
    "\n",
    "        input_shape: tuple showing size of input: (batch_size, channels, rows, cols) -> (m, Nc, Nh, Nw)\n",
    "        '''\n",
    "        \n",
    "        self.padding = Padding2D(p=p)\n",
    "        \n",
    "        self.F = filters\n",
    "        \n",
    "        self.input_shape_x = input_shape\n",
    "        \n",
    "        if type(kernel_size)==int:\n",
    "            self.kernel_size = (kernel_size, kernel_size)\n",
    "        elif type(kernel_size)==tuple and len(kernel_size)==2:\n",
    "            self.kernel_size = kernel_size\n",
    "            \n",
    "        self.Kh, self.Kw  = self.kernel_size\n",
    "        \n",
    "        if type(s)==int:\n",
    "            self.s = (s,s)\n",
    "        elif type(s)==tuple and len(s)==2:\n",
    "            self.s = s\n",
    "        \n",
    "        self.sh, self.sw = self.s\n",
    "        \n",
    "        self.activation = Activation(activation_type=activation_type)\n",
    "        self.use_bias = use_bias\n",
    "        self.weight_initializer_type = weight_initializer_type # none is handled\n",
    "        if kernel_regularizer is None:\n",
    "            self.kernel_regularizer = ('L2', 0)\n",
    "        else:\n",
    "            self.kernel_regularizer = kernel_regularizer\n",
    "        self.seed = seed\n",
    "        \n",
    "    def get_dimensions(self, input_shape):\n",
    "        \n",
    "        self.input_shape_x = input_shape # (3D or 4D)\n",
    "        \n",
    "        # Padded X will be actual input to this Conv2D\n",
    "        \n",
    "        self.input_shape, _ = self.padding.get_dimensions(self.input_shape_x, \n",
    "                                                          self.kernel_size, self.s)\n",
    "        \n",
    "        if len(input_shape)==3:\n",
    "            self.Nc, self.Nh, self.Nw = self.input_shape\n",
    "        elif len(input_shape)==4:\n",
    "            self.m, self.Nc, self.Nh, self.Nw = self.input_shape\n",
    "        \n",
    "        # Output shape\n",
    "        self.Oh = (self.Nh - self.Kh)//self.sh + 1\n",
    "        self.Ow = (self.Nw - self.Kw)//self.sw + 1\n",
    "        \n",
    "        if len(input_shape)==3:\n",
    "            self.output_shape = (self.F, self.Oh, self.Ow)\n",
    "        elif len(input_shape)==4:\n",
    "            self.output_shape = (self.m, self.F, self.Oh, self.Ow)\n",
    "\n",
    "    def initialize_parameters(self, input_shape, optimizer_type):\n",
    "        \n",
    "        self.get_dimensions(input_shape)\n",
    "        \n",
    "        shape_b = (self.F, self.Oh, self.Ow)\n",
    "        \n",
    "        shape_K = (self.F, self.Nc, self.Kh, self.Kw)\n",
    "        \n",
    "        initializer = Weights_initializer(shape=shape_K,\n",
    "                                          initializer_type=self.weight_initializer_type,\n",
    "                                          seed=self.seed)\n",
    "        \n",
    "        self.K = initializer.get_initializer()\n",
    "        self.b = np.zeros(shape_b)\n",
    "        \n",
    "        self.optimizer = Optimizer(optimizer_type=optimizer_type, shape_W=shape_K, shape_b=shape_b)\n",
    "    \n",
    "    def dilate2D(self, X, Dr=(1,1)):\n",
    "        dh, dw = Dr # Dilate rate\n",
    "        m, C, H, W = X.shape\n",
    "        Xd = np.insert(arr=X, obj=np.repeat(np.arange(1,W), dw-1), values=0, axis=-1)\n",
    "        Xd = np.insert(arr=Xd, obj=np.repeat(np.arange(1,H), dh-1), values=0, axis=-2)\n",
    "        return Xd\n",
    "\n",
    "    def prepare_subMatrix(self, X, Kh, Kw, s):\n",
    "        m, Nc, Nh, Nw = X.shape\n",
    "        sh, sw = s\n",
    "\n",
    "        Oh = (Nh-Kh)//sh + 1\n",
    "        Ow = (Nw-Kw)//sw + 1\n",
    "\n",
    "        strides = (Nc*Nh*Nw, Nw*Nh, Nw*sh, sw, Nw, 1)\n",
    "        strides = tuple(i * X.itemsize for i in strides)\n",
    "\n",
    "        subM = np.lib.stride_tricks.as_strided(X, \n",
    "                                               shape=(m, Nc, Oh, Ow, Kh, Kw),\n",
    "                                               strides=strides)\n",
    "\n",
    "        return subM\n",
    "         \n",
    "    def convolve(self, X, K, s=(1,1), mode='front'):\n",
    "\n",
    "        F, Kc, Kh, Kw = K.shape\n",
    "        subM = self.prepare_subMatrix(X, Kh, Kw, s)\n",
    "        \n",
    "        if mode=='front':\n",
    "            return np.einsum('fckl,mcijkl->mfij', K, subM)\n",
    "        elif mode=='back':\n",
    "            return np.einsum('fdkl,mcijkl->mdij', K, subM)\n",
    "        elif mode=='param':\n",
    "            return np.einsum('mfkl,mcijkl->fcij', K, subM)\n",
    "\n",
    "    def dZ_D_dX(self, dZ_D, Nh, Nw):\n",
    "\n",
    "        # Pad the dilated dZ (dZ_D -> dZ_Dp)\n",
    "\n",
    "        _, _, Hd, Wd = dZ_D.shape\n",
    "\n",
    "        ph = Nh - Hd + self.Kh - 1\n",
    "        pw = Nw - Wd + self.Kw - 1\n",
    "        \n",
    "        padding_back = Padding2D(p=(ph, pw))\n",
    "\n",
    "        dZ_Dp = padding_back.forward(dZ_D, self.kernel_size, self.s)\n",
    "\n",
    "        # Rotate K by 180 degrees\n",
    "        \n",
    "        K_rotated = self.K[:, :, ::-1, ::-1]\n",
    "        \n",
    "        # convolve dZ_Dp with K_rotated\n",
    "        \n",
    "        dXp = self.convolve(dZ_Dp, K_rotated, mode='back')\n",
    "        \n",
    "        dX = self.padding.backpropagation(dXp)\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def forward(self, X):\n",
    "        # padding\n",
    "\n",
    "        self.X = X\n",
    "\n",
    "        Xp = self.padding.forward(X, self.kernel_size, self.s)\n",
    "        \n",
    "        # convolve Xp with K\n",
    "        Z = self.convolve(Xp, self.K, self.s) + self.b\n",
    "    \n",
    "        a = self.activation.forward(Z)\n",
    "        \n",
    "        return a\n",
    "     \n",
    "    def backpropagation(self, da):\n",
    "\n",
    "        Xp = self.padding.forward(self.X, self.kernel_size, self.s)\n",
    "\n",
    "        m, Nc, Nh, Nw = Xp.shape\n",
    "\n",
    "        dZ = self.activation.backpropagation(da)\n",
    "        \n",
    "        # Dilate dZ (dZ-> dZ_D)\n",
    "        \n",
    "        dZ_D = self.dilate2D(dZ, Dr=self.s)\n",
    "        \n",
    "        dX = self.dZ_D_dX(dZ_D, Nh, Nw)\n",
    "        \n",
    "        # Gradient dK\n",
    "        \n",
    "        _, _, Hd, Wd = dZ_D.shape\n",
    "        \n",
    "        ph = self.Nh - Hd - self.Kh + 1\n",
    "        pw = self.Nw - Wd - self.Kw + 1\n",
    "\n",
    "        padding_back = Padding2D(p=(ph, pw))\n",
    "\n",
    "        dZ_Dp = padding_back.forward(dZ_D, self.kernel_size, self.s)\n",
    "        \n",
    "        self.dK = self.convolve(Xp, dZ_Dp, mode='param')\n",
    "        \n",
    "        # Gradient db\n",
    "        \n",
    "        self.db = np.sum(dZ, axis=0)\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    def update(self, lr, m, k):\n",
    "        '''\n",
    "        Parameters:\n",
    "\n",
    "        lr: learning rate\n",
    "        m: batch_size (sumber of samples in batch)\n",
    "        k: iteration_number\n",
    "        '''\n",
    "        dK, db = self.optimizer.get_optimization(self.dK, self.db, k)\n",
    "\n",
    "        if self.kernel_regularizer[0].lower()=='l2':\n",
    "            dK += self.kernel_regularizer[1] * self.K\n",
    "        elif self.weight_regularizer[0].lower()=='l1':\n",
    "            dK += self.kernel_regularizer[1] * np.sign(self.K)\n",
    "\n",
    "        self.K -= self.dK*(lr/m)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.b -= self.db*(lr/m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d0fab-5f0e-49dd-a8be-a324ad038c69",
   "metadata": {
    "id": "0d0d0fab-5f0e-49dd-a8be-a324ad038c69"
   },
   "source": [
    "#### [Maxpool2D class](https://pythonandml.github.io/dlbook/content/convolutional_neural_networks/pooling_layers.html)\n",
    "\n",
    "This class will perform forward and backpropagation for a Pooling layer on a batch of 2D image with channels.\n",
    "\n",
    "Type of Pooling available are: `Max` and `Mean`\n",
    "\n",
    "![](images/pooling_layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60991fd8-1e00-4020-85c0-b05fb24bea35",
   "metadata": {
    "id": "60991fd8-1e00-4020-85c0-b05fb24bea35",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Pooling2D:\n",
    "\n",
    "    def __init__(self, pool_size=(2,2), s=(2,2), p='valid', pool_type='max'):\n",
    "        '''\n",
    "        Parameters:\n",
    "        \n",
    "        pool_size: An integer or tuple/list of 2 integers, \n",
    "                     specifying the height and width of the 2D convolution window.\n",
    "                     \n",
    "        s: strides along height and width (sh, sw)\n",
    "        \n",
    "        p: padding type\n",
    "           Allowed types are only 'same', 'valid', an integer or a tuple of length 2.\n",
    "\n",
    "        pool_type: pooling type\n",
    "        Allowed types are only 'max', or 'mean'\n",
    "        '''\n",
    "        self.padding = Padding2D(p=p)\n",
    "        \n",
    "        if type(pool_size)==int:\n",
    "            self.pool_size = (pool_size, pool_size)\n",
    "        elif type(pool_size)==tuple and len(pool_size)==2:\n",
    "            self.pool_size = pool_size\n",
    "            \n",
    "        self.Kh, self.Kw  = self.pool_size\n",
    "        \n",
    "        if type(s)==int:\n",
    "            self.s = (s,s)\n",
    "        elif type(s)==tuple and len(s)==2:\n",
    "            self.s = s\n",
    "        \n",
    "        self.sh, self.sw = self.s\n",
    "        \n",
    "        self.pool_type = pool_type    \n",
    "\n",
    "    def get_dimensions(self, input_shape):\n",
    "        \n",
    "        if len(input_shape)==4:\n",
    "            m, Nc, Nh, Nw = input_shape\n",
    "        elif len(input_shape)==3:\n",
    "            Nc, Nh, Nw = input_shape\n",
    "\n",
    "        Oh = (Nh-self.Kh)//self.sh + 1\n",
    "        Ow = (Nw-self.Kw)//self.sw + 1\n",
    "        \n",
    "        if len(input_shape)==4:\n",
    "            self.output_shape = (m, Nc, Oh, Ow)\n",
    "        elif len(input_shape)==3:\n",
    "            self.output_shape = (Nc, Oh, Ow)\n",
    "\n",
    "    def prepare_subMatrix(self, X, pool_size, s):\n",
    "        m, Nc, Nh, Nw = X.shape\n",
    "        sh, sw = s\n",
    "        Kh, Kw = pool_size\n",
    "\n",
    "        Oh = (Nh-Kh)//sh + 1\n",
    "        Ow = (Nw-Kw)//sw + 1\n",
    "\n",
    "        strides = (Nc*Nh*Nw, Nh*Nw, Nw*sh, sw, Nw, 1)\n",
    "        strides = tuple(i * X.itemsize for i in strides)\n",
    "\n",
    "        subM = np.lib.stride_tricks.as_strided(X, \n",
    "                                               shape=(m, Nc, Oh, Ow, Kh, Kw),\n",
    "                                               strides=strides)\n",
    "        return subM\n",
    "    \n",
    "    def pooling(self, X, pool_size=(2,2), s=(2,2)):\n",
    "        \n",
    "        subM = self.prepare_subMatrix(X, pool_size, s)\n",
    "        \n",
    "        if self.pool_type=='max':\n",
    "            return np.max(subM, axis=(-2,-1))\n",
    "        elif self.pool_type=='mean':\n",
    "            return np.mean(subM, axis=(-2,-1))\n",
    "        else:\n",
    "            raise ValueError(\"Allowed pool types are only 'max' or 'mean'.\")\n",
    "\n",
    "    def prepare_mask(self, subM, Kh, Kw):\n",
    "        \n",
    "        m, Nc, Oh, Ow, Kh, Kw = subM.shape\n",
    "\n",
    "        a = subM.reshape(-1,Kh*Kw)\n",
    "        idx = np.argmax(a, axis=1)\n",
    "        b = np.zeros(a.shape)\n",
    "        b[np.arange(b.shape[0]), idx] = 1\n",
    "        mask = b.reshape((m, Nc, Oh, Ow, Kh, Kw))\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def mask_dXp(self, mask, Xp, dZ, Kh, Kw):\n",
    "        dA = np.einsum('i,ijk->ijk', dZ.reshape(-1), mask.reshape(-1,Kh,Kw)).reshape(mask.shape)\n",
    "        m, Nc, Nh, Nw = Xp.shape\n",
    "        strides = (Nc*Nh*Nw, Nh*Nw, Nw, 1)\n",
    "        strides = tuple(i * Xp.itemsize for i in strides)\n",
    "        dXp = np.lib.stride_tricks.as_strided(dA, Xp.shape, strides)\n",
    "        return dXp\n",
    "            \n",
    "    def maxpool_backprop(self, dZ, X):\n",
    "        \n",
    "        Xp = self.padding.forward(X, self.pool_size, self.s)\n",
    "\n",
    "        subM = self.prepare_subMatrix(Xp, self.pool_size, self.s)\n",
    "\n",
    "        m, Nc, Oh, Ow, Kh, Kw = subM.shape\n",
    "        \n",
    "        m, Nc, Nh, Nw = Xp.shape\n",
    "\n",
    "        mask = self.prepare_mask(subM, Kh, Kw)\n",
    "        \n",
    "        dXp = self.mask_dXp(mask, Xp, dZ, Kh, Kw)\n",
    "        \n",
    "        return dXp\n",
    "\n",
    "    def dZ_dZp(self, dZ):\n",
    "        sh, sw = self.s\n",
    "        Kh, Kw = self.pool_size\n",
    "\n",
    "        dZp = np.kron(dZ, np.ones((Kh,Kw), dtype=dZ.dtype)) # similar to repelem in matlab\n",
    "\n",
    "        jh, jw = Kh-sh, Kw-sw # jump along height and width\n",
    "\n",
    "        if jw!=0:\n",
    "            L = dZp.shape[-1]-1\n",
    "\n",
    "            l1 = np.arange(sw, L)\n",
    "            l2 = np.arange(sw + jw, L + jw)\n",
    "\n",
    "            mask = np.tile([True]*jw + [False]*jw, len(l1)//jw).astype(bool)\n",
    "\n",
    "            r1 = l1[mask[:len(l1)]]\n",
    "            r2 = l2[mask[:len(l2)]]\n",
    "\n",
    "            dZp[:, :, :, r1] += dZp[:, :, :, r2]\n",
    "            dZp = np.delete(dZp, r2, axis=-1)\n",
    "\n",
    "        if jh!=0:\n",
    "            L = dZp.shape[-2]-1\n",
    "\n",
    "            l1 = np.arange(sh, L)\n",
    "            l2 = np.arange(sh + jh, L + jh)\n",
    "\n",
    "            mask = np.tile([True]*jh + [False]*jh, len(l1)//jh).astype(bool)\n",
    "\n",
    "            r1 = l1[mask[:len(l1)]]\n",
    "            r2 = l2[mask[:len(l2)]]\n",
    "\n",
    "            dZp[:, :, r1, :] += dZp[:, :, r2, :]\n",
    "            dZp = np.delete(dZp, r2, axis=-2)\n",
    "\n",
    "        return dZp\n",
    "\n",
    "    def averagepool_backprop(self, dZ, X):\n",
    "        \n",
    "        Xp = self.padding.forward(X, self.pool_size, self.s)\n",
    "\n",
    "        m, Nc, Nh, Nw = Xp.shape\n",
    "\n",
    "        dZp = self.dZ_dZp(dZ)\n",
    "\n",
    "        ph = Nh - dZp.shape[-2]\n",
    "        pw = Nw - dZp.shape[-1]\n",
    "        \n",
    "        padding_back = Padding2D(p=(ph, pw))\n",
    "        \n",
    "        dXp = padding_back.forward(dZp, s=self.s, kernel_size=self.pool_size)\n",
    "\n",
    "        return dXp / (Nh*Nw)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Parameters:\n",
    "        \n",
    "        X: input of shape (m, Nc, Nh, Nw)\n",
    "        \n",
    "        Returns: \n",
    "        \n",
    "        Z: pooled X\n",
    "        '''\n",
    "\n",
    "        self.X = X\n",
    "\n",
    "        # padding\n",
    "        Xp = self.padding.forward(X, self.pool_size, self.s)\n",
    "\n",
    "        Z = self.pooling(Xp, self.pool_size, self.s)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def backpropagation(self, dZ):\n",
    "        '''\n",
    "        Parameters: \n",
    "        \n",
    "        dZ: Output Error\n",
    "        \n",
    "        Return:\n",
    "        \n",
    "        dX: Backprop Error of X\n",
    "        '''\n",
    "        if self.pool_type=='max':\n",
    "            dXp = self.maxpool_backprop(dZ, self.X)\n",
    "        elif self.pool_type=='mean':\n",
    "            dXp = self.averagepool_backprop(dZ, self.X)\n",
    "        dX = self.padding.backpropagation(dXp)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b3130f-b690-495f-93d6-70acf9440ea9",
   "metadata": {
    "id": "17b3130f-b690-495f-93d6-70acf9440ea9"
   },
   "source": [
    "#### Flatten class\n",
    "\n",
    "Converts 4D image $(m, N_c, N_h, N_w)$ to a 2D array of shape $(m, N_c \\times N_h \\times N_w)$ so that it can be sent to the Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d937d108-e4f5-4c96-8ae9-ad169f9bc6d9",
   "metadata": {
    "id": "d937d108-e4f5-4c96-8ae9-ad169f9bc6d9",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.m, self.Nc, self.Nh, self.Nw = X.shape\n",
    "        X_flat = X.reshape((self.m, self.Nc * self.Nh * self.Nw))\n",
    "        return X_flat\n",
    "     \n",
    "    def backpropagation(self, dZ):\n",
    "        dX = dZ.reshape((self.m, self.Nc, self.Nh, self.Nw))\n",
    "        return dX\n",
    "    \n",
    "    def get_dimensions(self, input_shape):\n",
    "        \n",
    "        if len(input_shape)==4:\n",
    "            self.m, self.Nc, self.Nh, self.Nw = input_shape\n",
    "        elif len(input_shape)==3:\n",
    "            self.Nc, self.Nh, self.Nw = input_shape\n",
    "        \n",
    "        self.output_shape = self.Nc * self.Nh * self.Nw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14923bea-70cc-452a-8c9e-80c336253328",
   "metadata": {
    "id": "14923bea-70cc-452a-8c9e-80c336253328"
   },
   "source": [
    "#### CNN\n",
    "\n",
    "This class finally contains the compile, summary, fit, predict, etc methods for executing our CNN model. Apart from `network_architecture`, `summary` and `initialize_parameters` all other functions are almost same to that of the functions used in [MLP model](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/neural_networks_mlp_scratch_best.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c192ddf2-d5b4-452b-b4c2-5616a681ec1f",
   "metadata": {
    "id": "c192ddf2-d5b4-452b-b4c2-5616a681ec1f",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class CNN:\n",
    "\n",
    "    def __init__(self, layers=None):\n",
    "        '''\n",
    "        This is a sequential CNN model\n",
    "        '''\n",
    "        if layers is None:\n",
    "            self.layers = []\n",
    "        else:\n",
    "            self.layers = layers\n",
    "        self.network_architecture_called = False\n",
    "\n",
    "    def add(self, layer):\n",
    "        # adds a layer to CNN model\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def Input(self, input_shape):\n",
    "        self.d = input_shape\n",
    "        self.architecture = [self.d] # output architecture\n",
    "        self.layer_name = [\"Input\"]\n",
    "    \n",
    "    def network_architecture(self):\n",
    "        for layer in self.layers:\n",
    "            if layer.__class__.__name__=='Conv2D':\n",
    "                if layer.input_shape_x is not None:\n",
    "                    self.Input(layer.input_shape_x)\n",
    "                layer.get_dimensions(self.architecture[-1])\n",
    "                self.architecture.append(layer.output_shape)\n",
    "                self.layer_name.append(layer.__class__.__name__)\n",
    "            elif layer.__class__.__name__ in ['Flatten', 'Pooling2D']:\n",
    "                layer.get_dimensions(self.architecture[-1])\n",
    "                self.architecture.append(layer.output_shape)\n",
    "                self.layer_name.append(layer.__class__.__name__)\n",
    "            elif layer.__class__.__name__=='Dense':\n",
    "                self.architecture.append(layer.neurons)\n",
    "                self.layer_name.append(layer.__class__.__name__)\n",
    "            else:\n",
    "                self.architecture.append(self.architecture[-1])\n",
    "                self.layer_name.append(layer.__class__.__name__)\n",
    "\n",
    "        self.layers = [layer for layer in self.layers if layer is not None]\n",
    "        try:\n",
    "            idx = model.layer_name.index(\"NoneType\")\n",
    "            del model.layer_name[idx]\n",
    "            del model.architecture[idx]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def summary(self):\n",
    "        if self.network_architecture_called==False:\n",
    "            self.network_architecture()\n",
    "            self.network_architecture_called = True\n",
    "        len_assigned = [45, 26, 15]\n",
    "        count = {'Dense': 1, 'Activation': 1, 'Input': 1,\n",
    "                'BatchNormalization': 1, 'Dropout': 1, 'Conv2D': 1,\n",
    "                'Pooling2D': 1, 'Flatten': 1}\n",
    "\n",
    "        col_names = ['Layer (type)', 'Output Shape', '# of Parameters']\n",
    "\n",
    "        print(\"Model: CNN\")\n",
    "        print('-'*sum(len_assigned))\n",
    "\n",
    "        text = ''\n",
    "        for i in range(3):\n",
    "            text += col_names[i] + ' '*(len_assigned[i]-len(col_names[i]))\n",
    "        print(text)\n",
    "\n",
    "        print('='*sum(len_assigned))\n",
    "\n",
    "        total_params = 0\n",
    "        trainable_params = 0\n",
    "        non_trainable_params = 0\n",
    "\n",
    "        for i in range(len(self.layer_name)):\n",
    "            # layer name\n",
    "            layer_name = self.layer_name[i]\n",
    "            name = layer_name.lower() + '_' + str(count[layer_name]) + ' ' + '(' + layer_name + ')'\n",
    "            count[layer_name] += 1\n",
    "\n",
    "            # output shape\n",
    "            try:\n",
    "                out = '(None, ' \n",
    "                for n in range(len(model.architecture[i])-1):\n",
    "                    out += str(model.architecture[i][n]) + ', '\n",
    "                out += str(model.architecture[i][-1]) + ')'\n",
    "            except:\n",
    "                out = '(None, ' + str(self.architecture[i]) + ')'\n",
    "\n",
    "            # number of params\n",
    "            if layer_name=='Dense':\n",
    "                h0 = self.architecture[i-1]\n",
    "                h1 = self.architecture[i]\n",
    "                if self.layers[i-1].use_bias:\n",
    "                    params = h0*h1 + h1\n",
    "                else:\n",
    "                    params = h0*h1\n",
    "                total_params += params\n",
    "                trainable_params += params\n",
    "            elif layer_name=='BatchNormalization':\n",
    "                h = self.architecture[i]\n",
    "                params = 4*h\n",
    "                trainable_params += 2*h\n",
    "                non_trainable_params += 2*h\n",
    "                total_params += params\n",
    "            elif layer_name=='Conv2D':\n",
    "                layer = self.layers[i-1]\n",
    "                if layer.use_bias:\n",
    "                    add_b = 1\n",
    "                else:\n",
    "                    add_b = 0\n",
    "                params = ((layer.Nc * layer.Kh * layer.Kw) + add_b) * layer.F\n",
    "                trainable_params += params\n",
    "                total_params += params\n",
    "            else:\n",
    "                # Pooling, Dropout, Flatten, Input\n",
    "                params = 0\n",
    "            names = [name, out, str(params)]\n",
    "\n",
    "            # print this row\n",
    "            text = ''\n",
    "            for j in range(3):\n",
    "                text += names[j] + ' '*(len_assigned[j]-len(names[j]))\n",
    "            print(text)\n",
    "            if i!=(len(self.layer_name)-1):\n",
    "                print('-'*sum(len_assigned))\n",
    "            else:\n",
    "                print('='*sum(len_assigned))\n",
    "\n",
    "        print(\"Total params:\", total_params)\n",
    "        print(\"Trainable params:\", trainable_params)\n",
    "        print(\"Non-trainable params:\", non_trainable_params)\n",
    "        print('-'*sum(len_assigned))\n",
    "\n",
    "    def compile(self, cost_type, optimizer_type):\n",
    "        self.cost = Cost(cost_type)\n",
    "        self.cost_type = cost_type\n",
    "        self.optimizer_type = optimizer_type\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        if self.network_architecture_called==False:\n",
    "            self.network_architecture()\n",
    "            self.network_architecture_called = True\n",
    "        # initialize parameters for different layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if layer.__class__.__name__ in ['Dense', 'Conv2D']:\n",
    "                layer.initialize_parameters(self.architecture[i], self.optimizer_type)\n",
    "            elif layer.__class__.__name__=='BatchNormalization':\n",
    "                layer.initialize_parameters(self.architecture[i])\n",
    "\n",
    "    def fit(self, X, y, epochs=10, batch_size=5, lr=1, X_val=None, y_val=None, verbose=1, lr_decay=None, **kwargs):\n",
    "        \n",
    "        self.history = {'Training Loss': [],\n",
    "                        'Validation Loss': [],\n",
    "                        'Training Accuracy': [],\n",
    "                        'Validation Accuracy': []}\n",
    "\n",
    "        iterations = 0\n",
    "        self.m = batch_size\n",
    "        self.initialize_parameters()\n",
    "\n",
    "        total_num_batches = np.ceil(len(X)/batch_size)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            cost_train = 0\n",
    "            num_batches = 0\n",
    "            y_pred_train = []\n",
    "            y_train = []\n",
    "            \n",
    "            print('\\nEpoch: ' + str(epoch+1) + '/' + str(epochs))\n",
    "\n",
    "            for i in tqdm(range(0, len(X), batch_size)):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                \n",
    "                Z = X_batch.copy()\n",
    "\n",
    "                # feed-forward\n",
    "                for layer in self.layers:\n",
    "                    Z = layer.forward(Z)\n",
    "\n",
    "                # calculating training accuracy\n",
    "                if self.cost_type=='cross-entropy':\n",
    "                    y_pred_train += np.argmax(Z, axis=1).tolist()\n",
    "                    y_train += np.argmax(y_batch, axis=1).tolist()\n",
    "                \n",
    "                # calculating the loss\n",
    "                cost_train += self.cost.get_cost(Z, y_batch) / self.m\n",
    "\n",
    "                # calculating dL/daL (last layer backprop error)\n",
    "                dZ = self.cost.get_d_cost(Z, y_batch)\n",
    "                \n",
    "                # backpropagation\n",
    "                for layer in self.layers[::-1]:\n",
    "                    dZ = layer.backpropagation(dZ)\n",
    "\n",
    "                # Parameters update\n",
    "                for layer in self.layers:\n",
    "                    if layer.__class__.__name__ in ['Dense', 'BatchNormalization', 'Conv2D']:\n",
    "                        layer.update(lr, self.m, iterations)\n",
    "\n",
    "                # Learning rate decay\n",
    "                if lr_decay is not None:\n",
    "                    lr = lr_decay(iterations, **kwargs)\n",
    "\n",
    "                num_batches += 1\n",
    "                iterations += 1\n",
    "            \n",
    "            cost_train /= num_batches\n",
    "\n",
    "            # printing purpose only (Training Accuracy, Validation loss and accuracy)\n",
    "\n",
    "            text  = 'Training Loss: ' + str(round(cost_train, 4)) + ' - '\n",
    "            self.history['Training Loss'].append(cost_train)\n",
    "\n",
    "            # training accuracy\n",
    "\n",
    "            if self.cost_type=='cross-entropy':\n",
    "                accuracy_train = np.sum(np.array(y_pred_train) == np.array(y_train)) / len(y_train)\n",
    "                text += 'Training Accuracy: ' + str(round(accuracy_train, 4))\n",
    "                self.history['Training Accuracy'].append(accuracy_train)\n",
    "            else:\n",
    "                text += 'Training Accuracy: ' + str(round(cost_train, 4))\n",
    "                self.history['Training Accuracy'].append(cost_train)\n",
    "            \n",
    "            if X_val is not None:\n",
    "                cost_val, accuracy_val = self.evaluate(X_val, y_val, batch_size)\n",
    "                text += ' - Validation Loss: ' + str(round(cost_val, 4)) + ' - '\n",
    "                self.history['Validation Loss'].append(cost_val)\n",
    "                text += 'Validation Accuracy: ' + str(round(accuracy_val, 4))\n",
    "                self.history['Validation Accuracy'].append(accuracy_val)\n",
    "\n",
    "            if verbose:\n",
    "                print(text)\n",
    "            else:\n",
    "                print()\n",
    "\n",
    "    def evaluate(self, X, y, batch_size=None):\n",
    "        \n",
    "        if batch_size is None:\n",
    "            batch_size = len(X)\n",
    "\n",
    "        cost = 0\n",
    "        correct = 0\n",
    "        num_batches = 0\n",
    "        utility = Utility()\n",
    "        Y_1hot, _ = utility.onehot(y)\n",
    "\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            X_batch = X[i:i+batch_size]\n",
    "            y_batch = y[i:i+batch_size]\n",
    "            Y_1hot_batch = Y_1hot[i:i+batch_size]\n",
    "            Z = X_batch.copy()\n",
    "            for layer in self.layers:\n",
    "                if layer.__class__.__name__=='BatchNormalization':\n",
    "                    Z = layer.forward(Z, mode='test')\n",
    "                else:\n",
    "                    Z = layer.forward(Z)\n",
    "            if self.cost_type=='cross-entropy':\n",
    "                cost += self.cost.get_cost(Z, Y_1hot_batch) / len(y_batch)\n",
    "                y_pred = np.argmax(Z, axis=1).tolist()\n",
    "                correct += np.sum(y_pred == y_batch)\n",
    "            else:\n",
    "                cost += self.cost.get_cost(Z, y_batch) / len(y_batch)\n",
    "\n",
    "            num_batches += 1\n",
    "\n",
    "        if self.cost_type=='cross-entropy':\n",
    "            accuracy = correct / len(y)\n",
    "            cost /= num_batches\n",
    "            return cost, accuracy\n",
    "        else:\n",
    "            cost /= num_batches\n",
    "            return cost, cost\n",
    "\n",
    "    def loss_plot(self):\n",
    "        plt.plot(self.history['Training Loss'], 'k')\n",
    "        if len(self.history['Validation Loss'])>0:\n",
    "            plt.plot(self.history['Validation Loss'], 'r')\n",
    "            plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "            plt.title('Model Loss')\n",
    "        else:\n",
    "            plt.title('Training Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.show()\n",
    "\n",
    "    def accuracy_plot(self):\n",
    "        plt.plot(self.history['Training Accuracy'], 'k')\n",
    "        if len(self.history['Validation Accuracy'])>0:\n",
    "            plt.plot(self.history['Validation Accuracy'], 'r')\n",
    "            plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "            plt.title('Model Accuracy')\n",
    "        else:\n",
    "            plt.title('Training Accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X, batch_size=None):\n",
    "        \n",
    "        if batch_size==None:\n",
    "            batch_size = len(X)\n",
    "\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            X_batch = X[i:i+batch_size]\n",
    "            Z = X_batch.copy()\n",
    "            for layer in self.layers:\n",
    "                if layer.__class__.__name__=='BatchNormalization':\n",
    "                    Z = layer.forward(Z, mode='test')\n",
    "                else:\n",
    "                    Z = layer.forward(Z)\n",
    "            if i==0:\n",
    "                if self.cost_type=='cross-entropy':\n",
    "                    y_pred = np.argmax(Z, axis=1).tolist()\n",
    "                else:\n",
    "                    y_pred = Z\n",
    "            else:\n",
    "                if self.cost_type=='cross-entropy':\n",
    "                    y_pred += np.argmax(Z, axis=1).tolist()\n",
    "                else:\n",
    "                    y_pred = np.vstack((y_pred, Z))\n",
    "        \n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fe9928-c7ae-4b5f-99cc-1f48506d2da4",
   "metadata": {
    "id": "f4fe9928-c7ae-4b5f-99cc-1f48506d2da4"
   },
   "source": [
    "### Validating model using MNIST Dataset\n",
    "\n",
    "Check this [page](https://en.wikipedia.org/wiki/MNIST_database) (link to an external website) to know more about **MNIST dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db9490-4476-43d6-857f-7a7422a01acc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04db9490-4476-43d6-857f-7a7422a01acc",
    "outputId": "2110bf65-e2b2-402a-e6d7-b3731ae95efb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 28, 28), 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1], X_test.shape[2])\n",
    "\n",
    "samples = 5000\n",
    "\n",
    "X_train = X_train[:samples, :]/255\n",
    "X_test = X_test[:samples, :]/255\n",
    "\n",
    "y_train = y_train[:samples]\n",
    "y_test = y_test[:samples]\n",
    "\n",
    "utility = Utility()\n",
    "\n",
    "# train validation split\n",
    "X_train_new, X_val, y_train_new, y_val = utility.train_test_split(X_train, y_train, test_ratio=0.2, seed=42)\n",
    "\n",
    "Y_1hot_train, _ = utility.onehot(y_train_new)\n",
    "\n",
    "input_shape = X_train_new.shape[1:]\n",
    "output_dim = Y_1hot_train.shape[1]\n",
    "\n",
    "input_shape, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "975c1435-bff8-42b9-aa9a-7454481670d5",
   "metadata": {
    "id": "975c1435-bff8-42b9-aa9a-7454481670d5"
   },
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "\n",
    "model.add(model.Input(input_shape=input_shape))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), p='same', activation_type=\"relu\"))\n",
    "\n",
    "model.add(Pooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(output_dim, activation_type=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ZqvFjft3oQns",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZqvFjft3oQns",
    "outputId": "64b00a53-1f3c-45d4-af46-67dc574a7655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: CNN\n",
      "--------------------------------------------------------------------------------------\n",
      "Layer (type)                                 Output Shape              # of Parameters\n",
      "======================================================================================\n",
      "input_1 (Input)                              (None, 3, 32, 32)         0              \n",
      "--------------------------------------------------------------------------------------\n",
      "conv2d_1 (Conv2D)                            (None, 32, 32, 32)        2432           \n",
      "--------------------------------------------------------------------------------------\n",
      "pooling2d_1 (Pooling2D)                      (None, 32, 16, 16)        0              \n",
      "--------------------------------------------------------------------------------------\n",
      "flatten_1 (Flatten)                          (None, 8192)              0              \n",
      "--------------------------------------------------------------------------------------\n",
      "dropout_1 (Dropout)                          (None, 8192)              0              \n",
      "--------------------------------------------------------------------------------------\n",
      "dense_1 (Dense)                              (None, 10)                81930          \n",
      "======================================================================================\n",
      "Total params: 84362\n",
      "Trainable params: 84362\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b512e2d-023f-477f-a635-4cd27d71c4fd",
   "metadata": {
    "id": "1b512e2d-023f-477f-a635-4cd27d71c4fd"
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 10\n",
    "lr = 0.05\n",
    "\n",
    "model.compile(cost_type=\"cross-entropy\", optimizer_type=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nQLeDLye8C2S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQLeDLye8C2S",
    "outputId": "49cb863f-d98a-4c2a-9414-58ab897e23af",
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [06:06<00:00, 22.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.2061 - Training Accuracy: 0.3025 - Validation Loss: 1.6472 - Validation Accuracy: 0.454\n",
      "\n",
      "Epoch: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [05:43<00:00, 21.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.3552 - Training Accuracy: 0.535 - Validation Loss: 1.121 - Validation Accuracy: 0.648\n",
      "\n",
      "Epoch: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [05:36<00:00, 21.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.0117 - Training Accuracy: 0.6577 - Validation Loss: 0.9004 - Validation Accuracy: 0.694\n",
      "\n",
      "Epoch: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [05:50<00:00, 21.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7985 - Training Accuracy: 0.727 - Validation Loss: 0.7702 - Validation Accuracy: 0.751\n",
      "\n",
      "Epoch: 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [05:30<00:00, 20.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6912 - Training Accuracy: 0.766 - Validation Loss: 0.677 - Validation Accuracy: 0.77\n",
      "\n",
      "Epoch: 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [05:33<00:00, 20.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6056 - Training Accuracy: 0.7988 - Validation Loss: 0.6384 - Validation Accuracy: 0.799\n",
      "\n",
      "Epoch: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [05:35<00:00, 20.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5665 - Training Accuracy: 0.81 - Validation Loss: 0.572 - Validation Accuracy: 0.806\n",
      "\n",
      "Epoch: 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [05:30<00:00, 20.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5072 - Training Accuracy: 0.8355 - Validation Loss: 0.5284 - Validation Accuracy: 0.839\n",
      "\n",
      "Epoch: 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [05:31<00:00, 20.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4754 - Training Accuracy: 0.8448 - Validation Loss: 0.5172 - Validation Accuracy: 0.843\n",
      "\n",
      "Epoch: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [05:31<00:00, 20.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4364 - Training Accuracy: 0.8615 - Validation Loss: 0.501 - Validation Accuracy: 0.853\n"
     ]
    }
   ],
   "source": [
    "LR_decay = LearningRateDecay()\n",
    "\n",
    "model.fit(X_train_new, Y_1hot_train, epochs=epochs, batch_size=batch_size, lr=lr, X_val=X_val, \n",
    "          y_val=y_val, verbose=1, lr_decay=LR_decay.constant, lr_0=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RV9figNP8C2S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "RV9figNP8C2S",
    "outputId": "45caaa59-71e7-4779-8cb1-789e86fb699f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.loss_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mRyGU_Wx8C2S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "mRyGU_Wx8C2S",
    "outputId": "a2fc1ff2-bb65-4d5a-b3c2-7a450af6a94f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.accuracy_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nGCWRlUm8C2S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nGCWRlUm8C2S",
    "outputId": "4d7895d8-8570-46f4-8f04-83e797e727de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[403,   0,   6,   5,   3,  10,  11,   1,  13,   8],\n",
       "       [  0, 547,   5,   3,   3,   3,   3,   1,   6,   0],\n",
       "       [  7,  14, 401,  36,   6,   7,   6,  15,  37,   1],\n",
       "       [  1,   6,  15, 387,   1,  37,   5,  20,  23,   5],\n",
       "       [  2,   2,   4,   0, 418,   2,   8,   8,   8,  48],\n",
       "       [ 12,   4,   6,  38,  10, 304,  11,  10,  51,  10],\n",
       "       [ 11,  10,  15,   1,  12,  22, 383,   1,   7,   0],\n",
       "       [  1,  10,  18,  12,  11,   1,   0, 415,   4,  40],\n",
       "       [ 10,   7,  13,  31,  20,  26,   2,  11, 350,  19],\n",
       "       [  2,   5,   4,   9,  47,   7,   2,  25,  11, 408]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12VUatYM8C2T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12VUatYM8C2T",
    "outputId": "4e22f817-4461-4173-e88f-51a23cc9da48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate = 19.68\n",
      "Accuracy = 80.32\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Error Rate =',round((1-acc)*100, 2))\n",
    "print('Accuracy =',round((acc)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zhGbEethodQ5",
   "metadata": {
    "id": "zhGbEethodQ5"
   },
   "source": [
    "Losses are converging and with more complex model (along with more time complexity), we can achieve very high accuracy on **MNIST**. But for now, the model is working as expected! "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
