
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>2.6. Back Propagation &#8212; Oddly Satisfying Deep Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="2.7. Terminologies Part-2" href="terminologies_part_2.html" />
    <link rel="prev" title="2.5. Forward propagation" href="forward_propagation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Oddly Satisfying Deep Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  1. Preliminaries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../preliminaries/data_preprocessing.html">
   1.1. Data Preprocessing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preliminaries/performance_metrics.html">
   1.2. Performance Metrics for ML and DL models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  2. Multilayer Perceptrons
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="activation.html">
   2.1. Activation Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perceptron.html">
   2.2. Perceptron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="terminologies_part_1.html">
   2.3. Terminologies Part-1
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cost_functions.html">
   2.4. Cost functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="forward_propagation.html">
   2.5. Forward propagation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2.6. Back Propagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="terminologies_part_2.html">
   2.7. Terminologies Part-2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gradient_descent.html">
   2.8. Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regularization.html">
   2.9. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dropout.html">
   2.10. Dropout regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="batch_normalization.html">
   2.11. Batch Normalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="numerical_example_forward_backward_propagation.html">
   2.12. Numerical example Forward and Back pass
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="shortcut_to_calculate_forward_back_propagation.html">
   2.13. Shortcut to calculate forward pass and backpropagation across layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="neural_networks_mlp_scratch_best.html">
   2.14. MLP model from scratch in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mlp_pytorch.html">
   2.15. 4 step process to build MLP model using PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mlp_keras.html">
   2.16. MLP model using Tensorflow - Keras
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  3. Convolutional Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../convolutional_neural_networks/cnn_over_mlp.html">
   3.1. Convolutional Neural Networks over MLP
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../convolutional_neural_networks/cnn_architecture.html">
   3.2. Basic Architecture of CNN
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../convolutional_neural_networks/convolutional_layers.html">
     3.2.1. Convolutional layers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convolutional_neural_networks/forward_propagation_convolution.html">
     3.2.2 Forward Propagation Convolution layer (Vectorized)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convolutional_neural_networks/backpropagation_convolution.html">
     3.2.3 Backward Propagation Convolution layer (Vectorized)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convolutional_neural_networks/pooling_layers.html">
     3.2.4. Pooling layers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../convolutional_neural_networks/cnn_from_scratch.html">
   3.3. Convolutional Neural Networks from scratch in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../convolutional_neural_networks/cnn_pytorch.html">
   3.4. 4 step process to build a CNN model using PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../convolutional_neural_networks/cnn_keras.html">
   3.5. CNN model using Tensorflow - Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../convolutional_neural_networks/cnn_state_of_the_art.html">
   3.6. State of the art CNN models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  4. Word Embeddings
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../word_embeddings/traditional_word_embeddings.html">
   4.1. Traditional Word Embeddings
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../word_embeddings/static_word_embeddings.html">
   4.2. Static Word Embeddings
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../word_embeddings/word2vec.html">
     4.2.1. Word2Vec
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../word_embeddings/glove.html">
     4.2.2 GloVe
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../word_embeddings/fasttext.html">
     4.2.3. FastText
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../word_embeddings/contextual_word_embeddings.html">
   4.3. Contextual Word Embeddings
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../word_embeddings/elmo.html">
     4.3.1. Embeddings from Language Models (ELMo)
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/pythonandml/dlbook/master?urlpath=tree/content/multilayer_perceptrons/backpropagation.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/pythonandml/dlbook/blob/master/content/multilayer_perceptrons/backpropagation.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/pythonandml/dlbook"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/pythonandml/dlbook/issues/new?title=Issue%20on%20page%20%2Fcontent/multilayer_perceptrons/backpropagation.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/content/multilayer_perceptrons/backpropagation.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#derivation-using-chain-rule">
   Derivation using Chain rule
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#equations-summary">
   Equations summary
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>2.6. Back Propagation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#derivation-using-chain-rule">
   Derivation using Chain rule
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#equations-summary">
   Equations summary
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="back-propagation">
<h1>2.6. Back Propagation<a class="headerlink" href="#back-propagation" title="Permalink to this headline">#</a></h1>
<p>The backpropagation is the second step of the learning, which consists of injecting the error committed in the forward propagation phase (error while making predictions because the parameters are not completely trained yet) into the network in the reverse direction (from output layer to input layer) and update its parameters to perform better on the next iteration.</p>
<p>Today, the backpropagation algorithm is the workhorse of learning in neural networks. At the heart of backpropagation is an expression for the partial derivative of the cost function <span class="math notranslate nohighlight">\(J(W, b)\)</span> with respect to weight <span class="math notranslate nohighlight">\(W\)</span> (or bias <span class="math notranslate nohighlight">\(b\)</span>) in the network. The expression tells us how quickly the cost changes when we change the weights and biases.</p>
<p>Hence, the optimization of the cost function <span class="math notranslate nohighlight">\(J(W, b)\)</span> is needed and it is usually performed through a descent method.</p>
<section id="derivation-using-chain-rule">
<h2>Derivation using Chain rule<a class="headerlink" href="#derivation-using-chain-rule" title="Permalink to this headline">#</a></h2>
<blockquote>
<div><p><strong>Note:</strong> <span class="math notranslate nohighlight">\(\odot\)</span> means element wise multiplication (also called <strong>Hadamard product</strong>)</p>
</div></blockquote>
<p>Since we need to calculate the partial derivative of the <a class="reference external" href="https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/cost_functions.html">cost function</a> <span class="math notranslate nohighlight">\(J(W, b)\)</span> (link to previous chapter) with respect to weight <span class="math notranslate nohighlight">\(W\)</span> (or bias <span class="math notranslate nohighlight">\(b\)</span>), we can do it using chain rule.</p>
<p>Backpropagation can be summarized using four different equations (vectorized form) considering the <a class="reference external" href="https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/terminologies_part_1.html#notations-to-be-used">notations presented in terminologies part-1</a> (link to previous chapter).</p>
<blockquote>
<div><p><strong>Note:</strong> For any layer <span class="math notranslate nohighlight">\(l\)</span> (<span class="math notranslate nohighlight">\(l=1,2,...,L\)</span>), we call the <code class="docutils literal notranslate"><span class="pre">backpropagation</span> <span class="pre">error</span></code> (<span class="math notranslate nohighlight">\(\delta_l\)</span>) in that layer as the partial derivative of the cost function <span class="math notranslate nohighlight">\(J(W, b)\)</span> with respect to weighted sum of that layer <span class="math notranslate nohighlight">\(z_l\)</span>. That is:</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[
\delta_l = \frac{\partial J(W, b)}{\partial z_l}
\]</div>
<p><strong>Output layer Backpropagation Error <span class="math notranslate nohighlight">\((\delta_L)\)</span></strong></p>
<p>For the output layer (<span class="math notranslate nohighlight">\(l=L\)</span>), we know from <a class="reference external" href="https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/forward_propagation.html">forward propagation</a> (link to previous chapter) that</p>
<div class="math notranslate nohighlight">
\[
z_L = a_{L-1}W_L + b_L^T 
\]</div>
<div class="math notranslate nohighlight">
\[
a_L = f_L(z_L)
\]</div>
<p>and the cost function <span class="math notranslate nohighlight">\(J(W,b)\)</span> is written using <span class="math notranslate nohighlight">\(a_L\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. So, using the chain rule,</p>
<div class="math notranslate nohighlight">
\[
\delta_L = \frac{\partial J(W, b)}{\partial z_L} = \frac{\partial J(W, b)}{\partial a_L} \odot \frac{\partial a_L}{\partial z_L}
\]</div>
<div class="math notranslate nohighlight">
\[\boxed{\therefore \delta_L = \frac{\partial J(W, b)}{\partial a_L} \odot f'_L(z_L)}\]</div>
<p><strong>Hidden layer Backpropagation Error <span class="math notranslate nohighlight">\((\delta_l)\)</span></strong></p>
<p>Now, for the hidden layers (<span class="math notranslate nohighlight">\(l=L-1, L-2,...,1\)</span>), we know from <a class="reference external" href="https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/forward_propagation.html">forward propagation</a> (link to previous chapter) that</p>
<div class="math notranslate nohighlight">
\[
z_l = a_{l-1}W_l + b_l^T 
\]</div>
<div class="math notranslate nohighlight">
\[
a_l = f_l(z_l)
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
z_{l+1} = a_lW_{l+1} + b_{l+1}^T 
\]</div>
<p>Therefore,</p>
<div class="math notranslate nohighlight">
\[
\delta_l = \frac{\partial J(W, b)}{\partial z_l} = \frac{\partial J(W, b)}{\partial a_l} \odot \frac{\partial a_l}{\partial z_l}
\]</div>
<p>We have</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial J(W, b)}{\partial a_l} = \frac{\partial J(W, b)}{\partial z_{l+1}} \frac{\partial z_{l+1}}{\partial a_l} 
\]</div>
<div class="math notranslate nohighlight">
\[
\therefore \frac{\partial J(W, b)}{\partial a_l} = \delta_{l+1} W_{l+1}^T
\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[
\boxed{\delta_l = (\delta_{l+1} W_{l+1}^T) \odot f'_l(z_l)}
\]</div>
<p><strong>Rate of change of the cost with respect to weights <span class="math notranslate nohighlight">\(W_l\)</span></strong></p>
<p>Now, the partial derivative of the cost function <span class="math notranslate nohighlight">\(J(W, b)\)</span> with respect to weight of that layer <span class="math notranslate nohighlight">\(W_l\)</span> will be:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial J(W, b)}{\partial W_l} = \frac{\partial J(W, b)}{\partial z_l} \frac{\partial z_l}{\partial W_l}
\]</div>
<div class="math notranslate nohighlight">
\[
\boxed{\therefore \frac{\partial J(W, b)}{\partial W_l} = a_{l-1}^T \delta_l}
\]</div>
<p><strong>Rate of change of the cost with respect to bias <span class="math notranslate nohighlight">\(b_l\)</span></strong></p>
<p>Finally, the partial derivative of the cost function <span class="math notranslate nohighlight">\(J(W, b)\)</span> with respect to bias of that layer <span class="math notranslate nohighlight">\(b_l\)</span> will be:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial J(W, b)}{\partial b_l} = \frac{\partial J(W, b)}{\partial z_l} \frac{\partial z_l}{\partial b_l}
\]</div>
<div class="math notranslate nohighlight">
\[
\boxed{\therefore \frac{\partial J(W, b)}{\partial b_l} = \sum \delta_l}
\]</div>
</section>
<section id="equations-summary">
<h2>Equations summary<a class="headerlink" href="#equations-summary" title="Permalink to this headline">#</a></h2>
<ol class="simple">
<li><p><strong>Output layer Backpropagation error</strong> (<span class="math notranslate nohighlight">\(\delta_L\)</span>)</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\delta_L = \frac{\partial J(W, b)}{\partial a_L} \odot f'_L(z_L)
\]</div>
<ol class="simple">
<li><p><strong>Hidden layer Backpropagation error</strong> (<span class="math notranslate nohighlight">\(\delta_l\)</span>)</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\delta_l = (\delta_{l+1} W_{l+1}^T) \odot f'_l(z_l)
\]</div>
<ol class="simple">
<li><p><strong>Rate of change of the cost with respect to weights <span class="math notranslate nohighlight">\(W_l\)</span></strong></p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\frac{\partial J(W, b)}{\partial W_l} = a_{l-1}^T \delta_l
\]</div>
<ol class="simple">
<li><p><strong>Rate of change of the cost with respect to bias <span class="math notranslate nohighlight">\(b_l\)</span></strong></p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\frac{\partial J(W, b)}{\partial b_l} = \sum \delta_l
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sum \delta_l\)</span> is summation over all the samples of <span class="math notranslate nohighlight">\(\delta_l\)</span>. Size of <span class="math notranslate nohighlight">\(\delta_l\)</span> is <span class="math notranslate nohighlight">\((m, h_l)\)</span>. So, we sum along the column (i.e. sum of column-1 then sum of column-2 and so on) to get a vector of size <span class="math notranslate nohighlight">\((h_l, 1)\)</span> which is same as <span class="math notranslate nohighlight">\(b_l\)</span>.</p>
<p>Also, <span class="math notranslate nohighlight">\(Q^T\)</span> denotes the transpose of any matrix <span class="math notranslate nohighlight">\(Q\)</span>. Thatâ€™s it. We now use these gradients to update the parameters weight <span class="math notranslate nohighlight">\(W\)</span> and bias <span class="math notranslate nohighlight">\(b\)</span>.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/multilayer_perceptrons"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="forward_propagation.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">2.5. Forward propagation</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="terminologies_part_2.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">2.7. Terminologies Part-2</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Ujjwal Khandelwal<br/>
  
      &copy; Copyright 2023.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>