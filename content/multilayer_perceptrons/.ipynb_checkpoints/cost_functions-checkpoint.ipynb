{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MpAhw6KAf00"
   },
   "source": [
    "# 2.4. Cost functions\n",
    "\n",
    "The goal of a Neural network model (or any model) is to make as accurate predictions ($\\hat{y}$) as possible. For this, we'd like an algorithm which lets us find the parameters (**weights** and **biases**) of the model so that the output from the network ($\\hat{y}$) approximates $y$ for the training input $X$. \n",
    "\n",
    "To quantify how well we're achieving this goal we define a *cost function* $J(W,b)$ (which is also called as *Loss* or *Objective* function) which is a function of the parameters weights $W$ and biases $b$. In simple terms, a cost function is a measure of error between what value your model predicts ($\\hat{y}$) and what the value actually is ($y$) and our goal is to minimize this error.\n",
    "\n",
    "We will only discuss about 2 types of cost functions (one for regression and one for classification).\n",
    "\n",
    "**Cost function for regression**\n",
    "\n",
    "In case of regression problem, we define cost function to be similar to the **Mean Square Error (MSE)**.\n",
    "\n",
    "Mathematically it is expressed as,\n",
    "\n",
    "$$\n",
    "J(W,b) = \\frac{1}{2} \\sum_{i=1}^m ||\\hat{y}_i-y_i||^2\n",
    "$$\n",
    "\n",
    "where $||z||$ is the $L2$ norm of $z$. The partial derivative of $J(W,b)$ with respect to $\\hat{y}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(W,b)}{\\partial \\hat{y}} = \\hat{y} - y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoMqw85DQsJT"
   },
   "source": [
    "**Example**\n",
    "\n",
    "Suppose,\n",
    "\n",
    "$$\n",
    "a = \\begin{bmatrix}\n",
    "1 & 2\\\\ \n",
    "2 & 0\\\\ \n",
    "3 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix}\n",
    "1 & 1\\\\ \n",
    "1 & 2\\\\ \n",
    "4 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, for $i=1, 2, 3$\n",
    "\n",
    "$$\n",
    "||a_i-y_i|| = \\begin{bmatrix}\n",
    "\\left \\| [0, 1] \\right \\|\\\\ \n",
    "\\left \\| [1, -2] \\right \\|\\\\ \n",
    "\\left \\| [-1, -2] \\right \\|\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1\\\\ \n",
    "\\sqrt{5}\\\\ \n",
    "\\sqrt{5} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "||a_i-y_i||^2 = \\begin{bmatrix}\n",
    "1\\\\ \n",
    "5\\\\ \n",
    "5 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "J(W,b) = \\frac{1}{2}\\sum_{i=1}^3||a_i-y_i||^2 = \\frac{1}{2}(5+5+1) = 5.5 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72EMinHa1FqK"
   },
   "source": [
    "The *mse(a,y)* function implemented below computes the cost function for regression where $a$ is the predicted output (same as $\\hat{y}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPkNkBKqAcEO"
   },
   "outputs": [],
   "source": [
    "def mse(a, y):\n",
    "    '''\n",
    "    Parameters\n",
    "    \n",
    "    a: Predicted output array of shape (m, c)\n",
    "    y: Actual output array of shape (m, c)\n",
    "    '''\n",
    "    return (1/2)*np.sum((np.linalg.norm(a-y, axis=1))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvQT-hRi2RkP"
   },
   "source": [
    "Let us test this function on the dataset from the above example. Complete solution is listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YmbJ6DCmQqRO",
    "outputId": "f3c9ad10-7fdf-4b02-c5c3-dee722af74ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost = J(W, b) = 5.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mse(a, y):\n",
    "    '''\n",
    "    Parameters\n",
    "    \n",
    "    a: Predicted output array of shape (m, c)\n",
    "    y: Actual output array of shape (m, c)\n",
    "    '''\n",
    "    return (1/2)*np.sum((np.linalg.norm(a-y, axis=1))**2)\n",
    "\n",
    "a = np.array([[1,2],\n",
    "              [2,0],\n",
    "              [3,1]])\n",
    "\n",
    "y = np.array([[1,1],\n",
    "             [1,2],\n",
    "             [4,3]])\n",
    "\n",
    "cost = mse(a, y)\n",
    "\n",
    "print(\"Cost = J(W, b) =\", round(cost, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vy5ovyon3ZfG"
   },
   "source": [
    "#### Validating using sklearn's MSE function\n",
    "\n",
    "> **Note:** Sklearn's mean square implementation is slightly different. It divides the cost function $J(W,b)$ by the number of samples $m$ as well.\n",
    "\n",
    "$$\n",
    "J(W,b) = \\frac{1}{2m} \\sum_{i=1}^m ||\\hat{y}_i-y_i||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFdDxHBm3FxZ",
    "outputId": "4f693026-075b-4534-945e-2779f2dd8b4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost (our function) = J(W, b) = 5.5\n",
      "Cost (sklearn) = J(W, b) = 5.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def mse(a, y):\n",
    "    '''\n",
    "    Parameters\n",
    "    \n",
    "    a: Predicted output array of shape (m, c)\n",
    "    y: Actual output array of shape (m, c)\n",
    "    '''\n",
    "    return (1/2)*np.sum((np.linalg.norm(a-y, axis=1))**2)\n",
    "\n",
    "a = np.array([[1,2],\n",
    "              [2,0],\n",
    "              [3,1]])\n",
    "\n",
    "y = np.array([[1,1],\n",
    "             [1,2],\n",
    "             [4,3]])\n",
    "\n",
    "m = y.shape[0]\n",
    "cost = mse(a, y)\n",
    "\n",
    "print(\"Cost (our function) = J(W, b) =\", round(cost, 3))\n",
    "print(\"Cost (sklearn) = J(W, b) =\", mean_squared_error(a, y)*m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wg8d7EOpL_FV"
   },
   "source": [
    "*d_mse(a, y)* function implemented below computes the partial derivative of the cost function with $\\hat{y} = a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BR7-5ymMT-N"
   },
   "outputs": [],
   "source": [
    "def d_mse(a, y):\n",
    "    return a - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFhkjT8A4mTP"
   },
   "source": [
    "**Cost function for classification**\n",
    "\n",
    "Categorical Cross-entropy cost (also called as log loss) can be used as a loss function when optimizing classification models like neural networks.\n",
    "\n",
    "Let $c$ be the number of classes in the target variable $y$ and $y$ is one-hot encoded. Let $a$ contains the Softmax probability (i.e. the final output obtained after passing it to softmax activation) in the output layer. \n",
    "\n",
    "Mathematically it is expressed as:\n",
    "\n",
    "$$\n",
    "J(W,b) = -\\sum_{i=1}^m \\sum_{j=1}^c y_j \\odot log(a_j)\n",
    "$$\n",
    "\n",
    "The partial derivative of $J(W,b)$ with respect to $\\hat{y} = a$ is ([check this link](https://stats.stackexchange.com/questions/277203/differentiation-of-cross-entropy)):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(W,b)}{\\partial \\hat{y}} = \\frac{\\partial J(W,b)}{\\partial a} = -\\frac{y}{a}\n",
    "$$\n",
    "\n",
    "**Softmax with Categorical cross entropy loss**\n",
    "\n",
    "It is interesting to note that in the case of softmax activation function with categorical cross entropy loss, if $a = \\text{softmax}(z)$, then the partial derivative of $J(W,b)$ with respect to $z$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(W,b)}{\\partial z} = a - y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKt0lenFEQ4P"
   },
   "source": [
    "Using the below example (in which we have $m=3$ samples and $c=4$ classes - target variable) we are testing the *cross_entropy(a, y)* function that we have implemented and validating the same using sklearn's log_loss module.\n",
    "\n",
    "> **Note:** Sklearn's log loss (cross entropy) implementation is slightly different. It divides the cost function $J(W,b)$ by the number of samples $m$ as well.\n",
    "\n",
    "$$\n",
    "J(W,b) = -\\frac{1}{m}\\sum_{i=1}^m \\sum_{j=1}^c y_j \\odot log(a_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVUN6t_qC_QR",
    "outputId": "d0571f18-511a-4af6-aeac-20d45eb02534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy cost (our function) = 4.199705077879927\n",
      "Cross Entropy cost (sklearn) = 4.199705077879927\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def cross_entropy(a, y, epsilon=1e-12):\n",
    "    '''\n",
    "    Parameters\n",
    "    \n",
    "    a: Predicted output array of shape (m, c)\n",
    "    y: Actual output array of shape (m, c)\n",
    "    '''\n",
    "    a = np.clip(a, epsilon, 1. - epsilon)\n",
    "    return -np.sum(y*np.log(a))\n",
    "\n",
    "a = np.array([[0.25,0.2,0.3,0.25],\n",
    "              [0.2,0.4,0.1,0.3],\n",
    "              [0.1,0.1,0.2,0.6]])\n",
    "\n",
    "y = np.array([[1,0,0,0],\n",
    "              [0,0,0,1],\n",
    "              [0,0,1,0]])\n",
    "\n",
    "m = y.shape[0]\n",
    "\n",
    "print(\"Cross Entropy cost (our function) =\", cross_entropy(a, y))\n",
    "print(\"Cross Entropy cost (sklearn) =\", log_loss(y, a)*m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtgosMkRMfrs"
   },
   "source": [
    "*d_cross_entropy(a, y)* function implemented below computes the partial derivative of the cost function with $\\hat{y} = a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxXqJgNwMh52"
   },
   "outputs": [],
   "source": [
    "def d_cross_entropy(a, y, epsilon=1e-12):\n",
    "    a = np.clip(a, epsilon, 1. - epsilon)\n",
    "    return -y/a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4imo5izPOR8j"
   },
   "source": [
    "Let us club all this together in a python class *Cost* whose constructor **init** has only one parameter (*cost_type*, which can either be 'mse' or 'cross-entropy', default value of 'mse')\n",
    "\n",
    "> **Note:** This class has a getter method for both (cost function and its partial derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SGaiow2OReo"
   },
   "outputs": [],
   "source": [
    "class Cost:\n",
    "\n",
    "    def __init__(self, cost_type='mse'):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        cost_type: type of cost function\n",
    "        available options are 'mse', and 'cross-entropy'\n",
    "        '''\n",
    "        self.cost_type = cost_type\n",
    "\n",
    "    def mse(self, a, y):\n",
    "        '''\n",
    "        Parameters\n",
    "        \n",
    "        a: Predicted output array of shape (m, d)\n",
    "        y: Actual output array of shape (m, d)\n",
    "        '''\n",
    "        return (1/2)*np.sum((np.linalg.norm(a-y, axis=1))**2)\n",
    "\n",
    "    def d_mse(self, a, y):\n",
    "        '''\n",
    "        dJ/da\n",
    "        '''\n",
    "        return a - y\n",
    "\n",
    "    def cross_entropy(self, a, y, epsilon=1e-12):\n",
    "        a = np.clip(a, epsilon, 1. - epsilon)\n",
    "        return -np.sum(y*np.log(a))\n",
    "\n",
    "    def d_cross_entropy(self, a, y, epsilon=1e-12):\n",
    "        a = np.clip(a, epsilon, 1. - epsilon)\n",
    "        return -y/a\n",
    "\n",
    "    def get_cost(self, a, y):\n",
    "        if self.cost_type == 'mse':\n",
    "            return self.mse(a, y)\n",
    "        elif self.cost_type == 'cross-entropy':\n",
    "            return self.cross_entropy(a, y)\n",
    "        else:\n",
    "            raise ValueError(\"Valid cost functions are only 'mse', and 'cross-entropy'\")\n",
    "\n",
    "    def get_d_cost(self, a, y):\n",
    "        if self.cost_type == 'mse':\n",
    "            return self.d_mse(a, y)\n",
    "        elif self.cost_type == 'cross-entropy':\n",
    "            return self.d_cross_entropy(a, y)\n",
    "        else:\n",
    "            raise ValueError(\"Valid cost functions are only 'mse', and 'cross-entropy'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofG-c2-KPLoR"
   },
   "source": [
    "Let us test this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5WCCJtSpPKyD",
    "outputId": "d9792b31-84d9-4725-8d5d-3c753cf5ce73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy cost = 4.199705077879927\n",
      "Cross Entropy cost (sklearn) = 4.199705077879927\n",
      "\n",
      "Cost derivative =\n",
      "\n",
      " [[-4.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.         -3.33333333]\n",
      " [ 0.          0.         -5.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "a = np.array([[0.25,0.2,0.3,0.25],\n",
    "              [0.2,0.4,0.1,0.3],\n",
    "              [0.1,0.1,0.2,0.6]])\n",
    "\n",
    "y = np.array([[1,0,0,0],\n",
    "              [0,0,0,1],\n",
    "              [0,0,1,0]])\n",
    "\n",
    "m = y.shape[0]\n",
    "\n",
    "cost = Cost(cost_type='cross-entropy')\n",
    "cost_value = cost.get_cost(a, y)\n",
    "dcost = cost.get_d_cost(a, y)\n",
    "\n",
    "print(\"Cross Entropy cost =\", cost_value)\n",
    "print(\"Cross Entropy cost (sklearn) =\", log_loss(y, a)*m)\n",
    "print(\"\\nCost derivative =\\n\\n\", dcost)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
