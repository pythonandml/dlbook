{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avUPq1o3XqIk"
   },
   "source": [
    "# Embeddings from Language Models (ELMo)\n",
    "\n",
    "ELMo is an NLP framework developed by [AllenNLP](https://allenai.org/allennlp/software/elmo). ELMo word vectors are calculated using a two-layer bidirectional language model (biLM). Each layer comprises forward and backward pass.\n",
    "\n",
    "Unlike traditional word embeddings such as [Word2Vec](https://pythonandml.github.io/dlbook/content/word_embeddings/word2vec.html) (link to previous chapter), [GloVe](https://pythonandml.github.io/dlbook/content/word_embeddings/glove.html) (link to previous chapter) or [FastText](https://pythonandml.github.io/dlbook/content/word_embeddings/fasttext.html) (link to previous chapter), the ELMo vector assigned to a token or word is actually a function of the entire sentence containing that word. Therefore, the same word can have different word vectors under different contexts as explained in [Contextual Word embeddings](https://pythonandml.github.io/dlbook/content/word_embeddings/contextual_word_embeddings.html) (link to previous chapter).\n",
    "\n",
    "### ELMO - Brief Overview\n",
    "\n",
    "Key idea:\n",
    "\n",
    "* Train a `forward LSTM-based language model` and a `backward LSTM-based language model` on some large corpus.\n",
    "* Use the hidden states of the LSTMs for each token to compute a vector representation of each word.\n",
    "\n",
    "A step in the pre-training process of ELMo: Given `\"Let's stick to\"` as input, predict the next most likely word – a language modeling task. When trained on a large dataset, the model starts to pick up on language patterns. It’s unlikely it’ll accurately guess the next word in this example. \n",
    "\n",
    "More realistically, after a word such as **hang**, it will assign a higher probability to a word like **out** (to spell `hang out`) than to **camera**.\n",
    "\n",
    "> We can see the hidden state of each unrolled-LSTM step peaking out from behind ELMo’s head. Those come in handy in the embedding proecss after this pre-training is done.\n",
    "\n",
    "**ELMo** actually goes a step further and trains a **bi-directional LSTM** – so that its language model doesn't only have a sense of the `next word`, but also the `previous word`.\n",
    "\n",
    "![](images/elmo_step1.png)\n",
    "\n",
    "**ELMo** comes up with the [contextualized embedding](https://pythonandml.github.io/dlbook/content/word_embeddings/contextual_word_embeddings.html) through grouping together the hidden states (and initial embedding) in a certain way (concatenation followed by weighted summation).\n",
    "\n",
    "![](images/elmo_step2.png)\n",
    "\n",
    "[Image source for both the images](https://jalammar.github.io/illustrated-bert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB5UjM26fT3H"
   },
   "source": [
    "### Implementation of ELMo word embeddings using Python-Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEPpzv0Ffs5O"
   },
   "source": [
    "Run these command before running the code in your terminal to install the necessary libraries.\n",
    "\n",
    "```\n",
    "pip install \"tensorflow>=2.0.0\"\n",
    "pip install --upgrade tensorflow-hub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbwPe-WJfzKO"
   },
   "source": [
    "#### import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qIb6JuCNf0up"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dv2EGSPhgJxp"
   },
   "source": [
    "#### Load pre trained ELMo model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fHMxT4cYgLia"
   },
   "outputs": [],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLAk96Z8gP4j"
   },
   "source": [
    "#### Create an instance of ELMo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PCorfNFZgWOR"
   },
   "outputs": [],
   "source": [
    "documents = [\"I will show you a valid point of reference and talk to the point\",\n",
    "\t\t         \"Where have you placed the point\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "L8lkeyDRgPtF"
   },
   "outputs": [],
   "source": [
    "embeddings = elmo(documents,\n",
    "\t                signature=\"default\",\n",
    "\t                as_dict=True)[\"elmo\"]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRpZCYdGhHLw"
   },
   "source": [
    "#### Print word embeddings for word point in given two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SXZ0KwKQWxJ0",
    "outputId": "1000ebb5-ca90-4eca-d89f-8069bca0b930"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings for the first 'point' in first sentence\n",
      "[-0.18793747 -0.23970088  0.3799212  ...  0.6024189   0.24705264\n",
      "  0.5594175 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Word embeddings for the first 'point' in first sentence\")\n",
    "print(sess.run(embeddings[0][6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kT2paq8HhJox",
    "outputId": "420900f7-af73-4b42-9dea-3f0b9dd9044a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings for the second 'point' in first sentence\n",
      "[ 0.37914592  0.00217749  0.8026248  ...  0.3467365  -0.11317912\n",
      "  0.32619435]\n"
     ]
    }
   ],
   "source": [
    "print(\"Word embeddings for the second 'point' in first sentence\")\n",
    "print(sess.run(embeddings[0][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GDpGAzpg10n",
    "outputId": "755a118c-811f-46af-9a20-87b49a20359d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings for 'point' in second sentence\n",
      "[-0.0284084  -0.04353216  0.04130162 ...  0.02583168 -0.01429837\n",
      " -0.01650422]\n"
     ]
    }
   ],
   "source": [
    "print(\"Word embeddings for 'point' in second sentence\")\n",
    "print(sess.run(embeddings[1][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UO5qQ488hRYG"
   },
   "source": [
    "The output shows different word embeddings for the same word **point** used in a different context in different sentences (also the embedding is different in case of same sentence but different context)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
