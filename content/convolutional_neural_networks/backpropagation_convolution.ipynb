{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NofXuSo8Kven"
   },
   "source": [
    "# 3.2.3 Backward Propagation Convolution layer (Vectorized)\n",
    "\n",
    "Now let us write (step by step) most general vectorized code using numpy (no loops will be used) to perform `backward propagation` on the convolution layer.\n",
    "\n",
    "> **Note:** The notations used can be found in the previous [section](https://pythonandml.github.io/dlbook/content/convolutional_neural_networks/convolutional_layers.html#notations) (link to previous section).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Procedure\n",
    "\n",
    "![](images/convolution_layer.png)\n",
    "\n",
    "We are given the error $dZ$ (partial derivative of the cost function $J$ with respect to the output $Z$) and we need to find $dX$, $dK$ and $db$ (Input and parameter gradients). I will not be going into the details of the derivation of Backpropagation (you can find it [here](https://medium.com/@mayank.utexas/backpropagation-for-convolution-with-strides-8137e4fc2710) in detail).\n",
    "\n",
    "#### Gradient $dX$\n",
    "\n",
    "In order to obtain $dX$, it turns out that the `backpropagation operation` is identical to a `stride = 1 convolution` of a **padded, dilated version of the output gradient $dZ$** with a **180 degrees rotated version of the filter $K$**\n",
    "\n",
    "This means, suppose following are the input matrix $X$, kernel matrix $K$ and output gradient of the layer $dZ$:\n",
    "\n",
    "![](images/backprop.png)\n",
    "\n",
    "Then, dilation of $dZ$ will be (with dilation rate $(d_h, d_w)=(s_h, s_w)$):\n",
    "\n",
    "![](images/dilated_dZ.png)\n",
    "\n",
    "Now, in order to pad the dilated $dZ$, we calculate the ($p_t$, $p_b$, $p_l$, $p_r$) (follow [notations](https://pythonandml.github.io/dlbook/content/convolutional_neural_networks/convolutional_layers.html#notations)) as follows:\n",
    "\n",
    "$p_h = N_h - D_h + K_h - 1$ and $p_w = N_w - D_w + K_w - 1$\n",
    "\n",
    "$$p_t = \\left \\lfloor \\frac{p_h}{2} \\right \\rfloor$$\n",
    "\n",
    "$$p_b = \\left \\lfloor \\frac{p_h+1}{2} \\right \\rfloor$$\n",
    "\n",
    "$$p_l = \\left \\lfloor \\frac{p_w}{2} \\right \\rfloor$$\n",
    "\n",
    "$$p_r = \\left \\lfloor \\frac{p_w+1}{2} \\right \\rfloor$$\n",
    "\n",
    "![](images/dilated_padded_dZ.png)\n",
    "\n",
    "Next, weâ€™re also going to make one modification to the filter (rotate it by 180 degrees):\n",
    "\n",
    "![](images/rot_180_K.png)\n",
    "\n",
    "Thats it! Just convolve this padded and dilated $dZ$ with the rotated Kernel (stride=1) and we have our $dX$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient $dK$\n",
    "\n",
    "In order to obtain $dK$, it turns out that the `backpropagation operation` is identical to a `stride = 1 convolution` operation of the **input $X$** with a **dilated version of the output gradient $dZ$**\n",
    "\n",
    "We already have our input $X$ and dilated output gradient $dZ$. Just convolve it with stride=1.\n",
    "\n",
    "![](images/input_dilated_dZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient db\n",
    "\n",
    "Just sum the gradient $dZ$ along the batch axis and we have $db$\n",
    "\n",
    "$$db = \\sum_{i=1}^m dZ_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code time\n",
    "\n",
    "#### Dilation\n",
    "\n",
    "Let us create a function `dilate2D(X, Dr)` (where **Dr is the dilation rate**) which dilates the matrix $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate2D(X, Dr):\n",
    "    dh, dw = Dr # Dilate rate\n",
    "    H, W = X.shape\n",
    "    Xd = np.insert(arr=X, obj=np.repeat(np.arange(1,W), dw-1), values=0, axis=1)\n",
    "    Xd = np.insert(arr=Xd, obj=np.repeat(np.arange(1,H), dh-1), values=0, axis=0)\n",
    "    return Xd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us test this function on some $X$ with `Dr=(2,3)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \n",
      "\n",
      " [[1 2 3 4 5]\n",
      " [4 5 6 7 8]\n",
      " [5 6 7 8 9]] \n",
      "\n",
      "Xd (Dilated) = \n",
      "\n",
      " [[1 0 0 2 0 0 3 0 0 4 0 0 5]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [4 0 0 5 0 0 6 0 0 7 0 0 8]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [5 0 0 6 0 0 7 0 0 8 0 0 9]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[1,2,3,4,5],\n",
    "              [4,5,6,7,8],\n",
    "              [5,6,7,8,9]])\n",
    "\n",
    "print('X = \\n\\n', X, '\\n')\n",
    "\n",
    "Dr = (2,3) # Dilation rate\n",
    "\n",
    "Xd = dilate2D(X, Dr)\n",
    "\n",
    "print('Xd (Dilated) = \\n\\n', Xd, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding a batch of tensor as input with multiple channels for dilation**\n",
    "\n",
    "The function for such an input will be `dilate2D_with_channels_batch(X, Dr)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate2D_with_channels_batch(X, Dr):\n",
    "    dh, dw = Dr # Dilate rate\n",
    "    m, C, H, W = X.shape\n",
    "    Xd = np.insert(arr=X, obj=np.repeat(np.arange(1,W), dw-1), values=0, axis=-1)\n",
    "    Xd = np.insert(arr=Xd, obj=np.repeat(np.arange(1,H), dh-1), values=0, axis=-2)\n",
    "    return Xd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \n",
      "\n",
      " [[[[9 4 0]\n",
      "   [1 9 0]\n",
      "   [1 8 9]]\n",
      "\n",
      "  [[0 8 6]\n",
      "   [4 3 0]\n",
      "   [4 6 8]]]\n",
      "\n",
      "\n",
      " [[[1 8 4]\n",
      "   [1 3 6]\n",
      "   [5 3 9]]\n",
      "\n",
      "  [[6 9 1]\n",
      "   [9 4 2]\n",
      "   [6 7 8]]]] \n",
      "\n",
      "Xd = \n",
      "\n",
      " [[[[9 0 0 4 0 0 0]\n",
      "   [0 0 0 0 0 0 0]\n",
      "   [1 0 0 9 0 0 0]\n",
      "   [0 0 0 0 0 0 0]\n",
      "   [1 0 0 8 0 0 9]]\n",
      "\n",
      "  [[0 0 0 8 0 0 6]\n",
      "   [0 0 0 0 0 0 0]\n",
      "   [4 0 0 3 0 0 0]\n",
      "   [0 0 0 0 0 0 0]\n",
      "   [4 0 0 6 0 0 8]]]\n",
      "\n",
      "\n",
      " [[[1 0 0 8 0 0 4]\n",
      "   [0 0 0 0 0 0 0]\n",
      "   [1 0 0 3 0 0 6]\n",
      "   [0 0 0 0 0 0 0]\n",
      "   [5 0 0 3 0 0 9]]\n",
      "\n",
      "  [[6 0 0 9 0 0 1]\n",
      "   [0 0 0 0 0 0 0]\n",
      "   [9 0 0 4 0 0 2]\n",
      "   [0 0 0 0 0 0 0]\n",
      "   [6 0 0 7 0 0 8]]]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "X = np.random.randint(0,10, size=(2,2,3,3))\n",
    "\n",
    "Dr = (2,3)\n",
    "\n",
    "Xd = dilate2D_with_channels_batch(X, Dr)\n",
    "\n",
    "print('X = \\n\\n', X, '\\n')\n",
    "\n",
    "print('Xd = \\n\\n', Xd, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation\n",
    "\n",
    "Now that we have dilation function and forward convolution function developed [here]()(link to previous chapter), we can easily backpropagate to obtain the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_input2D_with_channels_batch_and_many_filters(X, K, s, p='valid'):\n",
    "    \n",
    "    if type(p)==int:\n",
    "        m, Nc, Nh, Nw = X.shape\n",
    "        pt, pb = p, p\n",
    "        pl, pr = p, p\n",
    "        \n",
    "    elif type(p)==tuple:\n",
    "        m, Nc, Nh, Nw = X.shape\n",
    "        ph, pw = p\n",
    "        \n",
    "        pt, pb = ph//2, (ph+1)//2\n",
    "        pl, pr = pw//2, (pw+1)//2\n",
    "    \n",
    "    elif p=='valid':\n",
    "        return X\n",
    "    \n",
    "    elif p=='same':\n",
    "        m, Nc, Nh, Nw = X.shape\n",
    "        F, Kc, Kh, Kw = K.shape # F = number of filters\n",
    "        sh, sw = s\n",
    "\n",
    "        ph = (sh-1)*Nh + Kh - sh\n",
    "        pw = (sw-1)*Nw + Kw - sw\n",
    "\n",
    "        pt, pb = ph//2, (ph+1)//2\n",
    "        pl, pr = pw//2, (pw+1)//2\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Incorrect padding type. Allowed types are only 'same', 'valid', an integer or a tuple.\")\n",
    "\n",
    "    zeros_r = np.zeros((m, Nc, Nh, pr))\n",
    "    zeros_l = np.zeros((m, Nc, Nh, pl))\n",
    "    zeros_t = np.zeros((m, Nc, pt, Nw+pl+pr))\n",
    "    zeros_b = np.zeros((m, Nc, pb, Nw+pl+pr))\n",
    "\n",
    "    Xp = np.concatenate((X, zeros_r), axis=3)\n",
    "    Xp = np.concatenate((zeros_l, Xp), axis=3)\n",
    "    Xp = np.concatenate((zeros_t, Xp), axis=2)\n",
    "    Xp = np.concatenate((Xp, zeros_b), axis=2)\n",
    "\n",
    "    return Xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_with_channels_batch_and_many_filters(X, K, s, p='valid', mode='front'):\n",
    "    \n",
    "    # padding\n",
    "    Xp = pad_input2D_with_channels_batch_and_many_filters(X, K, s, p=p)\n",
    "    \n",
    "    m, Nc, Nh, Nw = Xp.shape\n",
    "    F, Kc, Kh, Kw = K.shape # F = number of filters\n",
    "    sh, sw = s # strides along height and width\n",
    "\n",
    "    Oh = (Nh-Kh)//sh + 1\n",
    "    Ow = (Nw-Kw)//sw + 1\n",
    "\n",
    "    strides = (Nc*Nh*Nw, Nw*Nh, Nw*sh, sw, Nw, 1)\n",
    "    strides = tuple(i * Xp.itemsize for i in strides)\n",
    "\n",
    "    subM = np.lib.stride_tricks.as_strided(Xp, shape=(m, Nc, Oh, Ow, Kh, Kw),\n",
    "                                            strides=strides)\n",
    "    \n",
    "    if mode=='front':\n",
    "        return np.einsum('fckl,mcijkl->mfij', K, subM)\n",
    "    elif mode=='back':\n",
    "        return np.einsum('fdkl,mcijkl->mdij', K, subM)\n",
    "    elif mode=='param':\n",
    "        return np.einsum('mfkl,mcijkl->fcij', K, subM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient $dX$**\n",
    "\n",
    "**Forward Propagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 2, 36, 47)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "X = np.random.randint(0,10, size=(256,1,36,47))\n",
    "\n",
    "# different Kernel for each channel and many such Kernels\n",
    "K = np.random.randint(0,10, size=(2,1,5,8))\n",
    "\n",
    "s = (7,4)\n",
    "\n",
    "Xp = pad_input2D_with_channels_batch_and_many_filters(X, K, s, p='same')\n",
    "\n",
    "Z = conv2d_with_channels_batch_and_many_filters(X, K, s, p='same')\n",
    "\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 2, 254, 199)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given Gradient of output dZ\n",
    "dZ = np.random.randint(0,10, size=Z.shape)\n",
    "\n",
    "# Dilate dZ\n",
    "dZ_D = dilate2D_with_channels_batch(dZ, Dr=s)\n",
    "\n",
    "# Pad the dilated dZ\n",
    "\n",
    "m, F, Hd, Wd = dZ_D.shape\n",
    "\n",
    "m, Nc, Nh, Nw = Xp.shape\n",
    "\n",
    "m, Nc, Nhx, Nwx = X.shape\n",
    "\n",
    "F, Kc, Kh, Kw = K.shape\n",
    "\n",
    "ph = Nh - Hd + Kh - 1\n",
    "pw = Nw - Wd + Kw - 1\n",
    "\n",
    "dZ_Dp = pad_input2D_with_channels_batch_and_many_filters(dZ_D, K, s, p=(ph, pw))\n",
    "\n",
    "dZ_Dp.shape # padding it to make its size same as Xp (padded X)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rotation by 180 degrees**\n",
    "\n",
    "This is just a one line implementation\n",
    "\n",
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K1 = np.arange(1,10).reshape(3,3)\n",
    "K1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9, 8, 7],\n",
       "       [6, 5, 4],\n",
       "       [3, 2, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K1_rotate_180 = K1[::-1, ::-1]\n",
    "K1_rotate_180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[7, 0, 3, 7, 0, 8, 2, 0],\n",
       "         [8, 7, 9, 5, 4, 1, 6, 6],\n",
       "         [1, 7, 2, 9, 0, 0, 1, 8],\n",
       "         [7, 8, 2, 1, 0, 0, 2, 6],\n",
       "         [3, 6, 6, 4, 8, 9, 0, 0]]],\n",
       "\n",
       "\n",
       "       [[[9, 0, 2, 5, 1, 0, 6, 6],\n",
       "         [9, 1, 5, 3, 8, 5, 7, 0],\n",
       "         [0, 9, 3, 0, 2, 9, 3, 4],\n",
       "         [6, 2, 1, 9, 6, 3, 8, 6],\n",
       "         [6, 5, 6, 0, 5, 7, 8, 3]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kr = K[:, :, ::-1, ::-1]\n",
    "Kr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally it is time for convolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = (1,1)\n",
    "\n",
    "dXp = conv2d_with_channels_batch_and_many_filters(dZ_Dp, Kr, s=sb, p='valid', mode='back')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1, 250, 192)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dXp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1, 250, 192)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1, 36, 47)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a small function such that we can extract the original input errors from the padding ones. This you can consider as **backpropagation through padding operation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding2D_backwards_with_channels_batch(X, Xp, dXp):\n",
    "    m, Nc, Nh, Nw = Xp.shape\n",
    "    m, Nc, Nhx, Nwx = X.shape\n",
    "    ph = Nh - Nhx\n",
    "    pw = Nw - Nwx\n",
    "    pt, pb = ph//2, (ph+1)//2\n",
    "    pl, pr = pw//2, (pw+1)//2\n",
    "    dX = dXp[:, :, pt:pt+Nhx, pl:pl+Nwx]\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 1, 36, 47)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dX = padding2D_backwards_with_channels_batch(X, Xp, dXp)\n",
    "dX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUIDmPkK1bs7"
   },
   "source": [
    "**Gradient $dK$**\n",
    "\n",
    "We have `dZ_D = Dilated output gradient dZ` and input $X$ (padded). Just convolve them with stride=1 to get $dK$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 5, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb = (1,1)\n",
    "\n",
    "ph = Nh - Hd - Kh + 1\n",
    "pw = Nw - Wd - Kw + 1\n",
    "\n",
    "dZ_Dp = pad_input2D_with_channels_batch_and_many_filters(dZ_D, K, s, p=(ph, pw))\n",
    "\n",
    "dK = conv2d_with_channels_batch_and_many_filters(Xp, dZ_Dp, s=sb, p='valid', mode='param')\n",
    "\n",
    "dK.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 5, 8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient db**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 36, 47)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = np.sum(dZ, axis=0)\n",
    "db.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Congrats! We have successfully calculated the gradients of the input, and the parameters for the convolution layer using only numpy (and no for loops)!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
