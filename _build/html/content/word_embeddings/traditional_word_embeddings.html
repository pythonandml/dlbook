
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>4.1. Traditional Word Embeddings &#8212; Oddly Satisfying Deep Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="4.2. Static Word Embeddings" href="static_word_embeddings.html" />
    <link rel="prev" title="3.6. State of the art CNN models" href="../convolutional_neural_networks/cnn_state_of_the_art.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Oddly Satisfying Deep Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  1. Preliminaries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../preliminaries/data_preprocessing.html">
   1.1. Data Preprocessing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preliminaries/performance_metrics.html">
   1.2. Performance Metrics for ML and DL models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  2. Multilayer Perceptrons
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../multilayer_perceptrons/activation.html">
   2.1. Activation Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multilayer_perceptrons/perceptron.html">
   2.2. Perceptron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multilayer_perceptrons/terminologies_part_1.html">
   2.3. Terminologies Part-1
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multilayer_perceptrons/cost_functions.html">
   2.4. Cost functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multilayer_perceptrons/forward_propagation.html">
   2.5. Forward propagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multilayer_perceptrons/backpropagation.html">
   2.6. Back Propagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multilayer_perceptrons/terminologies_part_2.html">
   2.7. Terminologies Part-2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multilayer_perceptrons/gradient_descent.html">
   2.8. Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multilayer_perceptrons/regularization.html">
   2.9. Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multilayer_perceptrons/dropout.html">
   2.10. Dropout regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multilayer_perceptrons/batch_normalization.html">
   2.11. Batch Normalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multilayer_perceptrons/numerical_example_forward_backward_propagation.html">
   2.12. Numerical example Forward and Back pass
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multilayer_perceptrons/shortcut_to_calculate_forward_back_propagation.html">
   2.13. Shortcut to calculate forward pass and backpropagation across layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multilayer_perceptrons/neural_networks_mlp_scratch_best.html">
   2.14. MLP model from scratch in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multilayer_perceptrons/mlp_pytorch.html">
   2.15. 4 step process to build MLP model using PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multilayer_perceptrons/mlp_keras.html">
   2.16. MLP model using Tensorflow - Keras
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  3. Convolutional Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../convolutional_neural_networks/cnn_over_mlp.html">
   3.1. Convolutional Neural Networks over MLP
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../convolutional_neural_networks/cnn_architecture.html">
   3.2. Basic Architecture of CNN
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../convolutional_neural_networks/convolutional_layers.html">
     3.2.1. Convolutional layers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convolutional_neural_networks/forward_propagation_convolution.html">
     3.2.2 Forward Propagation Convolution layer (Vectorized)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convolutional_neural_networks/backpropagation_convolution.html">
     3.2.3 Backward Propagation Convolution layer (Vectorized)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../convolutional_neural_networks/pooling_layers.html">
     3.2.4. Pooling layers
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../convolutional_neural_networks/cnn_from_scratch.html">
   3.3. Convolutional Neural Networks from scratch in Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../convolutional_neural_networks/cnn_pytorch.html">
   3.4. 4 step process to build a CNN model using PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../convolutional_neural_networks/cnn_keras.html">
   3.5. CNN model using Tensorflow - Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../convolutional_neural_networks/cnn_state_of_the_art.html">
   3.6. State of the art CNN models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  4. Word Embeddings
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   4.1. Traditional Word Embeddings
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="static_word_embeddings.html">
   4.2. Static Word Embeddings
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="word2vec.html">
     4.2.1. Word2Vec
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="glove.html">
     4.2.2 GloVe
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="fasttext.html">
     4.2.3. FastText
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="contextual_word_embeddings.html">
   4.3. Contextual Word Embeddings
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="elmo.html">
     4.3.1. Embeddings from Language Models (ELMo)
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/pythonandml/dlbook/master?urlpath=tree/content/word_embeddings/traditional_word_embeddings.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/pythonandml/dlbook/blob/master/content/word_embeddings/traditional_word_embeddings.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/pythonandml/dlbook"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/pythonandml/dlbook/issues/new?title=Issue%20on%20page%20%2Fcontent/word_embeddings/traditional_word_embeddings.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/content/word_embeddings/traditional_word_embeddings.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#one-hot-vectors">
   One-Hot Vectors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#defining-corpus">
   Defining Corpus
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#count-vector-or-bag-of-words-bow">
   Count-Vector or Bag-of-Words (BOW)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#co-occurrence-matrix">
   Co-occurrence Matrix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#n-gram">
   N-Gram
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tf-idf-with-code-from-scratch">
   TF-IDF (with code from scratch)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#term-frequency-tf-matrix">
     Term Frequency (TF) Matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inverse-document-frequency-idf">
     Inverse Document Frequency (IDF)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calculate-tf-idf">
     Calculate TF-IDF
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>4.1. Traditional Word Embeddings</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#one-hot-vectors">
   One-Hot Vectors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#defining-corpus">
   Defining Corpus
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#count-vector-or-bag-of-words-bow">
   Count-Vector or Bag-of-Words (BOW)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#co-occurrence-matrix">
   Co-occurrence Matrix
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#n-gram">
   N-Gram
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tf-idf-with-code-from-scratch">
   TF-IDF (with code from scratch)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#term-frequency-tf-matrix">
     Term Frequency (TF) Matrix
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inverse-document-frequency-idf">
     Inverse Document Frequency (IDF)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calculate-tf-idf">
     Calculate TF-IDF
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="traditional-word-embeddings">
<h1>4.1. Traditional Word Embeddings<a class="headerlink" href="#traditional-word-embeddings" title="Permalink to this headline">#</a></h1>
<p><strong>Word Embedding</strong> in simple term is a representation of a word. Word embedding techniques are used for analysing texts. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.</p>
<p>But why do we need it? The problem with <code class="docutils literal notranslate"><span class="pre">Machine</span> <span class="pre">Learning</span></code> or <code class="docutils literal notranslate"><span class="pre">Deep</span> <span class="pre">Learning</span></code> models is that it can’t understand texts. So, we have to convert the words into numbers.</p>
<blockquote>
<div><p>Thus, loosely speaking, word embedding are vector representations of a particular word. It allows words with similar meaning to have a similar representation. They can also approximate meanings.</p>
</div></blockquote>
<p>So, how do they capture the context? We will get to it soon. But in this section we will be describing various ways to find the vector representation of a word using the <code class="docutils literal notranslate"><span class="pre">Traditional</span> <span class="pre">Word</span> <span class="pre">Embeddings</span></code> approach.</p>
<section id="one-hot-vectors">
<h2>One-Hot Vectors<a class="headerlink" href="#one-hot-vectors" title="Permalink to this headline">#</a></h2>
<p>You can follow <a class="reference external" href="https://pythonandml.github.io/dlbook/content/preliminaries/data_preprocessing.html#one-hot-encoding">this write-up</a> (link to previous chapter) to know about <strong>One Hot Encoding</strong>.</p>
<p>Let us consider an example, where we are provided with the following sentence:</p>
<p><strong>Sentence</strong>: We are learning word embeddings.</p>
<p>If we construct an exhaustive vocabulary from this sentence (let’s call it <span class="math notranslate nohighlight">\(V\)</span>), it would have</p>
<p><code class="docutils literal notranslate"><span class="pre">V</span> <span class="pre">=</span> <span class="pre">{We,</span> <span class="pre">are,</span> <span class="pre">learning,</span> <span class="pre">word,</span> <span class="pre">embeddings}</span></code></p>
<p>If we construct one hot encoded vector for each of these words in V, then</p>
<div class="math notranslate nohighlight">
\[
\text{We} = \begin{bmatrix}
1, &amp; 0, &amp; 0, &amp; 0, &amp; 0
\end{bmatrix}
\]</div>
<div class="math notranslate nohighlight">
\[
\text{are} = \begin{bmatrix}
0, &amp; 1, &amp; 0, &amp; 0, &amp; 0
\end{bmatrix}
\]</div>
<div class="math notranslate nohighlight">
\[
\text{learning} = \begin{bmatrix}
0, &amp; 0, &amp; 1, &amp; 0, &amp; 0
\end{bmatrix}
\]</div>
<div class="math notranslate nohighlight">
\[
\text{word} = \begin{bmatrix}
0, &amp; 0, &amp; 0, &amp; 1, &amp; 0
\end{bmatrix}
\]</div>
<div class="math notranslate nohighlight">
\[
\text{embeddings} = \begin{bmatrix}
0, &amp; 0, &amp; 0, &amp; 0, &amp; 1
\end{bmatrix}
\]</div>
<p>Here, every word has been assigned a unique vector and the length of our one-hot encoded vector would be equal to the size of <span class="math notranslate nohighlight">\(V\)</span> <span class="math notranslate nohighlight">\((|V|=5)\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="simple">
<li><p>In one hot encoding representations, all the words are independent of each other, and therefore there is no relationship between words.</p></li>
<li><p>One hot encoding is computionally expensive because in reality the vocabulary size could be billions if not trillions and thus such representation is not feasible in real life scenario.</p></li>
</ol>
</div>
</section>
<section id="defining-corpus">
<h2>Defining Corpus<a class="headerlink" href="#defining-corpus" title="Permalink to this headline">#</a></h2>
<p>Let us assume, our <code class="docutils literal notranslate"><span class="pre">corpus</span></code> (meaning collection of written texts) comprises of 4 documents (here documents means sentences).</p>
<p><strong>Corpus</strong></p>
<blockquote>
<div><p>This is the first document. This document is the second document. This is the third one. Is this the first document?</p>
</div></blockquote>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<p class="sd-card-text">Document-1 (Sentence-1): This is the first document.</p>
<p class="sd-card-text">Document-2 (Sentence-2): This document is the second document.</p>
<p class="sd-card-text">Document-3 (Sentence-3): This is the third one.</p>
<p class="sd-card-text">Document-4 (Sentence-4): Is this the first document?</p>
</div>
</div>
<p>Now assume that we have removed all the punctuations from the documents and also we have converted all the words to lower case. This is what pre-processed documents would look like:</p>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<p class="sd-card-text">Document-1 (Sentence-1): this is the first document</p>
<p class="sd-card-text">Document-2 (Sentence-2): this document is the second document</p>
<p class="sd-card-text">Document-3 (Sentence-3): this is the third one</p>
<p class="sd-card-text">Document-4 (Sentence-4): is this the first document</p>
</div>
</div>
<p>Now, if we construct an exhaustive vocabulary from this pre-processed corpus (let’s call it <span class="math notranslate nohighlight">\(V\)</span>), it would have</p>
<p><code class="docutils literal notranslate"><span class="pre">V</span> <span class="pre">=</span> <span class="pre">{'document',</span> <span class="pre">'this',</span> <span class="pre">'one',</span> <span class="pre">'first',</span> <span class="pre">'second',</span> <span class="pre">'third',</span> <span class="pre">'is',</span> <span class="pre">'the'}</span></code></p>
</section>
<section id="count-vector-or-bag-of-words-bow">
<h2>Count-Vector or Bag-of-Words (BOW)<a class="headerlink" href="#count-vector-or-bag-of-words-bow" title="Permalink to this headline">#</a></h2>
<p>It is one of the simplest ways of doing text vectorization. It creates a <strong>document term matrix</strong>, that indicates if a particular word (or a token or a term) appears in the document. Each element (number) of this matrix represents the <code class="docutils literal notranslate"><span class="pre">frequency</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">word</span></code> in that particular document.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="text-align:right head"><p>document</p></th>
<th class="text-align:right head"><p>this</p></th>
<th class="text-align:right head"><p>one</p></th>
<th class="text-align:right head"><p>first</p></th>
<th class="text-align:right head"><p>second</p></th>
<th class="text-align:right head"><p>third</p></th>
<th class="text-align:right head"><p>is</p></th>
<th class="text-align:right head"><p>the</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Document-1</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Document-2</p></td>
<td class="text-align:right"><p>2</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Document-3</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Document-4</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
</tbody>
</table>
<p>Now, a column in the above matrix can also be understood as a word vector representation for the corresponding word.</p>
<p>For Example,</p>
<ul class="simple">
<li><p>Vector for document is [1, 2, 0, 1],</p></li>
<li><p>Vector for first is [1, 0, 0, 1], and so on.</p></li>
</ul>
<p>Hence we see that even though the length of the vocabulary <span class="math notranslate nohighlight">\(|V|=8\)</span>, still the vector size for each word turns out to be of length-4. This way we have converted the words into numbers.</p>
<p>Since the calculation is just based on count and there is no consideration for the context of a word, the method proves to be less beneficial.</p>
</section>
<section id="co-occurrence-matrix">
<h2>Co-occurrence Matrix<a class="headerlink" href="#co-occurrence-matrix" title="Permalink to this headline">#</a></h2>
<p>The co-occurrence matrix indicates how many times the row word (e.g. ‘is’ from the above corpus mentioned in <code class="docutils literal notranslate"><span class="pre">Defining</span> <span class="pre">Corpus</span></code> section) is surrounded (in a sentence, or in the <span class="math notranslate nohighlight">\(±2\)</span> sized word window which can vary depending on the application type) by the column word (e.g. ‘the’).</p>
<p><img alt="" src="../../_images/co_occurence.png" /></p>
<p>The entry ‘4’ in the following table (which is the co-occurrence matrix), means that we had 4 sentences in our text where <code class="docutils literal notranslate"><span class="pre">is</span></code> was surrounded by <code class="docutils literal notranslate"><span class="pre">the</span></code>.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="text-align:right head"><p>document</p></th>
<th class="text-align:right head"><p>first</p></th>
<th class="text-align:right head"><p>is</p></th>
<th class="text-align:right head"><p>one</p></th>
<th class="text-align:right head"><p>second</p></th>
<th class="text-align:right head"><p>the</p></th>
<th class="text-align:right head"><p>third</p></th>
<th class="text-align:right head"><p>this</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>document</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>2</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>4</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>first</p></td>
<td class="text-align:right"><p>2</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>2</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>is</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>4</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>4</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>one</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>second</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>the</p></td>
<td class="text-align:right"><p>4</p></td>
<td class="text-align:right"><p>2</p></td>
<td class="text-align:right"><p>4</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>3</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>third</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>this</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>4</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>3</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
</tr>
</tbody>
</table>
<p>Note that the co-occurrence matrix is always symmetric - the entry with the row word ‘the’ and the column word ‘is’ will be 4 as well (as these words co-occur in the very same sentences).</p>
</section>
<section id="n-gram">
<h2>N-Gram<a class="headerlink" href="#n-gram" title="Permalink to this headline">#</a></h2>
<p>Similar to the count vectorization technique, in the N-Gram method, a document term matrix is generated and each cell represents the count (frequency of the term in the document).</p>
<p>The difference in the N-grams method is that the count represents the combination of adjacent words of length <span class="math notranslate nohighlight">\(n\)</span> in the document.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Count vectorization is N-Gram where n=1.</p>
</div>
<p>For example, just consider only the <strong>document-1</strong> (from above example) for now.</p>
<p><strong>Document-1</strong>: <code class="docutils literal notranslate"><span class="pre">this</span> <span class="pre">is</span> <span class="pre">the</span> <span class="pre">first</span> <span class="pre">document</span></code></p>
<p><strong>Unigrams (n=1)</strong></p>
<p>If <span class="math notranslate nohighlight">\(n=1\)</span>, i.e unigrams, the word pairs would be <code class="docutils literal notranslate"><span class="pre">[&quot;this&quot;,</span> <span class="pre">&quot;is&quot;,</span> <span class="pre">&quot;the&quot;,</span> <span class="pre">&quot;first&quot;,</span> <span class="pre">&quot;document&quot;]</span></code> as we would expect in case of count vector (Bag-of-Words).</p>
<p><strong>Bigrams (n=2)</strong></p>
<p>If <span class="math notranslate nohighlight">\(n=2\)</span>, i.e bigrams, the word pairs would be <code class="docutils literal notranslate"><span class="pre">[&quot;this</span> <span class="pre">is&quot;,</span> <span class="pre">&quot;is</span> <span class="pre">the&quot;,</span> <span class="pre">&quot;the</span> <span class="pre">first&quot;,</span> <span class="pre">&quot;first</span> <span class="pre">document&quot;]</span></code></p>
<p><strong>Trigrams (n=3)</strong></p>
<p><img alt="" src="../../_images/trigrams.gif" /></p>
<p>If <span class="math notranslate nohighlight">\(n=3\)</span>, i.e trigrams, the word pairs would be <code class="docutils literal notranslate"><span class="pre">[&quot;this</span> <span class="pre">is</span> <span class="pre">the&quot;,</span> <span class="pre">&quot;is</span> <span class="pre">the</span> <span class="pre">first&quot;,</span> <span class="pre">&quot;the</span> <span class="pre">first</span> <span class="pre">document&quot;]</span></code></p>
<blockquote>
<div><p>Unlike <strong>BOW</strong>, it maintains word order but there are too many features and therefore it is computationally expensive. Also, choosing the optimal value of “<span class="math notranslate nohighlight">\(n\)</span>” is not that easy task.</p>
</div></blockquote>
</section>
<section id="tf-idf-with-code-from-scratch">
<h2>TF-IDF (with code from scratch)<a class="headerlink" href="#tf-idf-with-code-from-scratch" title="Permalink to this headline">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">TF-IDF</span></code> stands for Term Frequency-Inverse Document Frequency. It is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.</p>
<p>This method is an improvement over the <code class="docutils literal notranslate"><span class="pre">Count</span> <span class="pre">Vector</span></code> method as the frequency of a particular word is considered across the whole corpus and not just a single document.</p>
<p>The main idea is to give more weight to the words which are very specific to certain documents whereas to give less weight to the words which are more general and occur across most documents.</p>
<p>For example, if we look at the term document matrix formed in the Count Vector section,</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="text-align:right head"><p>document</p></th>
<th class="text-align:right head"><p>this</p></th>
<th class="text-align:right head"><p>one</p></th>
<th class="text-align:right head"><p>first</p></th>
<th class="text-align:right head"><p>second</p></th>
<th class="text-align:right head"><p>third</p></th>
<th class="text-align:right head"><p>is</p></th>
<th class="text-align:right head"><p>the</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Document-1</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Document-2</p></td>
<td class="text-align:right"><p>2</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Document-3</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Document-4</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>0</p></td>
<td class="text-align:right"><p>1</p></td>
<td class="text-align:right"><p>1</p></td>
</tr>
</tbody>
</table>
<p>just look at the words <code class="docutils literal notranslate"><span class="pre">{&quot;this&quot;,</span> <span class="pre">&quot;is&quot;,</span> <span class="pre">&quot;the&quot;}</span></code>, they occur in all the documents and therefore they are less relevant in finding context of the word in a particular document. If you say word <code class="docutils literal notranslate"><span class="pre">&quot;this&quot;</span></code> then it is difficult to figure out which document you are referring to whereas if you say <code class="docutils literal notranslate"><span class="pre">&quot;one&quot;</span></code>, then it is obvious that you are referring to <code class="docutils literal notranslate"><span class="pre">document-3</span></code>!</p>
<p>Let us see how to calculate this TF-IDF for given collection of documents (consider the same example as in Count-Vector section).</p>
<div class="sd-card sd-sphinx-override sd-mb-3 sd-shadow-sm docutils">
<div class="sd-card-body docutils">
<p class="sd-card-text">Document-1 (Sentence-1): this is the first document</p>
<p class="sd-card-text">Document-2 (Sentence-2): this document is the second document</p>
<p class="sd-card-text">Document-3 (Sentence-3): this is the third one</p>
<p class="sd-card-text">Document-4 (Sentence-4): is this the first document</p>
</div>
</div>
<p>If we construct an exhaustive vocabulary from this pre-processed corpus (<span class="math notranslate nohighlight">\(V\)</span>), it would have</p>
<p><code class="docutils literal notranslate"><span class="pre">V</span> <span class="pre">=</span> <span class="pre">{'second',</span> <span class="pre">'document',</span> <span class="pre">'is',</span> <span class="pre">'third',</span> <span class="pre">'one',</span> <span class="pre">'this',</span> <span class="pre">'the',</span> <span class="pre">'first'}</span></code></p>
<p>Let us code the same using python.</p>
<p>Suppose we have been provided with this list of documents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">documents_name</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Document-1&quot;</span><span class="p">,</span> <span class="s2">&quot;Document-2&quot;</span><span class="p">,</span> <span class="s2">&quot;Document-3&quot;</span><span class="p">,</span> <span class="s2">&quot;Document-4&quot;</span><span class="p">]</span>

<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;this is the first document&#39;</span><span class="p">,</span>
             <span class="s1">&#39;this document is the second document&#39;</span><span class="p">,</span>
             <span class="s1">&#39;this is the third one&#39;</span><span class="p">,</span>
             <span class="s1">&#39;is this the first document&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>In order to obtain the vocabulary <span class="math notranslate nohighlight">\(V\)</span>, we will first tokenize the above documents list (extract words from the sentences).</p>
<p><strong>Tokenize</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_tokens</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">document</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">):</span>
        <span class="n">words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="n">word_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="n">word_tokens</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[&#39;this&#39;, &#39;is&#39;, &#39;the&#39;, &#39;first&#39;, &#39;document&#39;],
 [&#39;this&#39;, &#39;document&#39;, &#39;is&#39;, &#39;the&#39;, &#39;second&#39;, &#39;document&#39;],
 [&#39;this&#39;, &#39;is&#39;, &#39;the&#39;, &#39;third&#39;, &#39;one&#39;],
 [&#39;is&#39;, &#39;this&#39;, &#39;the&#39;, &#39;first&#39;, &#39;document&#39;]]
</pre></div>
</div>
</div>
</div>
<p><strong>Create vocabulary list</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

<span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">word_tokens</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">document</span><span class="p">:</span>
        <span class="n">vocab</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocab (V) =&quot;</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;length of V (|V|) =&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Vocab (V) = [&#39;second&#39;, &#39;document&#39;, &#39;is&#39;, &#39;third&#39;, &#39;one&#39;, &#39;this&#39;, &#39;the&#39;, &#39;first&#39;]
length of V (|V|) = 8
</pre></div>
</div>
</div>
</div>
<section id="term-frequency-tf-matrix">
<h3>Term Frequency (TF) Matrix<a class="headerlink" href="#term-frequency-tf-matrix" title="Permalink to this headline">#</a></h3>
<p>We create a 2D matrix <code class="docutils literal notranslate"><span class="pre">TF</span></code> of size <span class="math notranslate nohighlight">\((D, W)\)</span> where:</p>
<ol class="simple">
<li><p>Each row represents the document (total number of documents = <span class="math notranslate nohighlight">\(D\)</span>, here <span class="math notranslate nohighlight">\(D=4\)</span>) and</p></li>
<li><p>Each column represents the word in <span class="math notranslate nohighlight">\(V\)</span> (total number of words in vocab = <span class="math notranslate nohighlight">\(|V| = W\)</span>, here <span class="math notranslate nohighlight">\(W=8\)</span>) and</p></li>
<li><p>The value in the <span class="math notranslate nohighlight">\(i^{th}\)</span> row and <span class="math notranslate nohighlight">\(j^{th}\)</span> column of this matrix is <span class="math notranslate nohighlight">\(\text{tf}_{ij}\)</span> where: <br>
<span class="math notranslate nohighlight">\(\text{tf}_{ij}\)</span> = count or frequency of the <span class="math notranslate nohighlight">\(j^{th}\)</span> word in the <span class="math notranslate nohighlight">\(i^{th}\)</span> document divided by the total number of words in <span class="math notranslate nohighlight">\(i^{th}\)</span> document.</p></li>
</ol>
<p>This TF matrix is also called <strong>Term Frequency</strong> matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">TF</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">))]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">document</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">document</span><span class="p">:</span>
        <span class="n">j</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="n">TF</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">TF</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">vocab</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">documents_name</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
  <div id="df-e2a394e1-1df4-4dfe-9450-14f554375389">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>second</th>
      <th>document</th>
      <th>is</th>
      <th>third</th>
      <th>one</th>
      <th>this</th>
      <th>the</th>
      <th>first</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Document-1</th>
      <td>0.000</td>
      <td>0.200</td>
      <td>0.200</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.200</td>
      <td>0.200</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>Document-2</th>
      <td>0.167</td>
      <td>0.333</td>
      <td>0.167</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.167</td>
      <td>0.167</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Document-3</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.200</td>
      <td>0.2</td>
      <td>0.2</td>
      <td>0.200</td>
      <td>0.200</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>Document-4</th>
      <td>0.000</td>
      <td>0.200</td>
      <td>0.200</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.200</td>
      <td>0.200</td>
      <td>0.2</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-e2a394e1-1df4-4dfe-9450-14f554375389')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-e2a394e1-1df4-4dfe-9450-14f554375389 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-e2a394e1-1df4-4dfe-9450-14f554375389');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
</div></div>
</div>
</section>
<section id="inverse-document-frequency-idf">
<h3>Inverse Document Frequency (IDF)<a class="headerlink" href="#inverse-document-frequency-idf" title="Permalink to this headline">#</a></h3>
<p><span class="math notranslate nohighlight">\(\text{df}\)</span> (Document Frequency) is the count of documents in which the word (term) is present. We consider one occurrence if the term is present in the document at least once, we do not need to know the number of times the term is present in the document.</p>
<p>It will be a vector whose length will be same as that of the vocabulary size (<span class="math notranslate nohighlight">\(|V|=W\)</span>). So, the <span class="math notranslate nohighlight">\(\text{idf}\)</span> for the <span class="math notranslate nohighlight">\(j^{th}\)</span> word will be given by</p>
<div class="math notranslate nohighlight">
\[
\text{idf}_j = \text{log}\frac{D}{\text{df}_j}
\]</div>
<p>Where <span class="math notranslate nohighlight">\(D\)</span> is the total number of documents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">idf</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))]</span>
<span class="n">D</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">)</span>

<span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">word_vocab</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">word_tokens</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">document</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">word_vocab</span> <span class="o">==</span> <span class="n">word</span><span class="p">:</span>
                <span class="n">df</span> <span class="o">+=</span> <span class="mi">1</span> 
                <span class="k">break</span>
    <span class="n">idf</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">D</span><span class="o">/</span><span class="n">df</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">idf</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="n">vocab</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
  <div id="df-043d3ac5-8315-49bf-9c82-d2c5cec9a194">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>second</th>
      <th>document</th>
      <th>is</th>
      <th>third</th>
      <th>one</th>
      <th>this</th>
      <th>the</th>
      <th>first</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.386</td>
      <td>0.288</td>
      <td>0.0</td>
      <td>1.386</td>
      <td>1.386</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.693</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-043d3ac5-8315-49bf-9c82-d2c5cec9a194')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-043d3ac5-8315-49bf-9c82-d2c5cec9a194 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-043d3ac5-8315-49bf-9c82-d2c5cec9a194');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
</div></div>
</div>
</section>
<section id="calculate-tf-idf">
<h3>Calculate TF-IDF<a class="headerlink" href="#calculate-tf-idf" title="Permalink to this headline">#</a></h3>
<p>This is the final step to form <code class="docutils literal notranslate"><span class="pre">TF-IDF</span></code> matrix of size <span class="math notranslate nohighlight">\((D, W)\)</span> and rows and columns of this matrix are same as the one described for <code class="docutils literal notranslate"><span class="pre">Term</span> <span class="pre">Frequency</span> <span class="pre">(TF)</span> <span class="pre">Matrix</span></code> section</p>
<div class="math notranslate nohighlight">
\[
\text{tfidf}_{ij} = \text{tf}_{ij} \times \text{idf}_j
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">TFIDF</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">))]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)):</span>
        <span class="n">TFIDF</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">TF</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">idf</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">TFIDF</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">vocab</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">documents_name</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
  <div id="df-58259ec5-8382-4242-9a55-74597990286f">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>second</th>
      <th>document</th>
      <th>is</th>
      <th>third</th>
      <th>one</th>
      <th>this</th>
      <th>the</th>
      <th>first</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Document-1</th>
      <td>0.000</td>
      <td>0.058</td>
      <td>0.0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.139</td>
    </tr>
    <tr>
      <th>Document-2</th>
      <td>0.231</td>
      <td>0.096</td>
      <td>0.0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>Document-3</th>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.0</td>
      <td>0.277</td>
      <td>0.277</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000</td>
    </tr>
    <tr>
      <th>Document-4</th>
      <td>0.000</td>
      <td>0.058</td>
      <td>0.0</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.139</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-58259ec5-8382-4242-9a55-74597990286f')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-58259ec5-8382-4242-9a55-74597990286f button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-58259ec5-8382-4242-9a55-74597990286f');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
</div></div>
</div>
<p>From the above table, we can see that <strong>TF-IDF</strong> of common words <code class="docutils literal notranslate"><span class="pre">(&quot;this&quot;,</span> <span class="pre">&quot;is&quot;,</span> <span class="pre">&quot;the&quot;)</span></code> is zero, which shows they are not significant.</p>
<p>On the other hand, the <strong>TF-IDF</strong> of <code class="docutils literal notranslate"><span class="pre">(“document”</span> <span class="pre">,</span> <span class="pre">&quot;one&quot;,</span> <span class="pre">&quot;second&quot;,</span> <span class="pre">&quot;first&quot;,</span> <span class="pre">&quot;third&quot;)</span></code> are non-zero. These words have more significance.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Each column of this matrix represents an individual unique word.</p>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/word_embeddings"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../convolutional_neural_networks/cnn_state_of_the_art.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">3.6. State of the art CNN models</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="static_word_embeddings.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">4.2. Static Word Embeddings</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Ujjwal Khandelwal<br/>
  
      &copy; Copyright 2023.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>