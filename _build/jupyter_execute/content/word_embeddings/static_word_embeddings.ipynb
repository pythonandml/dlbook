{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7aYyVZcyLNj"
   },
   "source": [
    "# 4.2. Static Word Embeddings\n",
    "\n",
    "In order to understand what word embeddings are, why do we need it and what are traditional word embeddings, please visit [this page](https://pythonandml.github.io/dlbook/content/word_embeddings/traditional_word_embeddings.html) (link to the previous chapter).\n",
    "\n",
    "In this section we will be describing various ways to find the vector representation of a word using the `Static Word Embeddings` approach.\n",
    "\n",
    "`Static Word embeddings` and [Contextual Word embeddings](https://pythonandml.github.io/dlbook/content/word_embeddings/contextual_word_embeddings.html) are slightly different.\n",
    "\n",
    "Word embeddings provided by **word2vec**, **Glove** or **fastText** has a vocabulary (dictionary) of words. The elements of this vocabulary (or dictionary) are words and its corresponding word embeddings. Hence, given a word, its embeddings is always the same in whichever sentence it occurs. Here, the pre-trained word embeddings are static. \n",
    "\n",
    "For example, consider the two sentences:\n",
    "\n",
    "1. I will show you a valid **point** of reference and talk to the **point**.\n",
    "\n",
    "2. Where have you placed the **point**.\n",
    "\n",
    "> Now, for the static word embeddings from a pre-trained embeddings such as [Word2Vec](https://pythonandml.github.io/dlbook/content/word_embeddings/word2vec.html) (link to previous chapter), the embeddings for the word **point** is same for both of its occurrences in example-1 and also the same for the word **point** in example-2. All three occurrences has same embeddings ([reference](https://stackoverflow.com/a/62314668/20878502)).\n",
    "\n",
    "However note that the context of a `single contextual word` is mostly preserved in this type of embedding. Let us explore further different types of static word embeddings.\n",
    "\n",
    "1. [Word2Vec](https://pythonandml.github.io/dlbook/content/word_embeddings/word2vec.html)\n",
    "\n",
    "2. [GloVe](https://pythonandml.github.io/dlbook/content/word_embeddings/glove.html)\n",
    "\n",
    "3. [FastText](https://pythonandml.github.io/dlbook/content/word_embeddings/fasttext.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWXF62tJMgmX"
   },
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "It is the most widely used method to compare two vectors. It is a dot product between two vectors. We would find the cosine angle between the two vectors. For degree 0, cosine is 1 and it is less than 1 for any other angle.\n",
    "\n",
    "![](images/similarity_cosine.webp)\n",
    "\n",
    "Let us compute cosine similarity between 2 vectors using sklearn's cosine similarity module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IHocr0U8QRll",
    "outputId": "fc76a1ed-40f8-4fb0-f3f3-e83703d856d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between A and B = [[0.4472136]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "A = [[1, 3]]\n",
    "B = [[-2, 2]]\n",
    "\n",
    "print(\"Cosine Similarity between A and B =\", cosine_similarity(A, B))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}