{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FIDYhh4Ktbd"
   },
   "source": [
    "# 4.2.2 GloVe\n",
    "\n",
    "`GloVe` stands for **Global Vectors for Word Representation**. This algorithm is an improvement over the [Word2Vec](https://pythonandml.github.io/dlbook/content/word_embeddings/word2vec.html) (link to previous chapter) approach as it considers global statistics instead of local statistics. \n",
    "\n",
    "Here, global statistics mean, the words considered from across the whole corpus. In this method, we take the corpus and iterate through it and get the [co-occurrence](https://pythonandml.github.io/dlbook/content/word_embeddings/traditional_word_embeddings.html#co-occurrence-matrix) (link to previous chapter) of each word with other words in the corpus.\n",
    "\n",
    "```{note}\n",
    "GloVe method is built on an important idea:\n",
    "\n",
    "You can derive semantic relationships between words from the co-occurrence matrix.\n",
    "```\n",
    "\n",
    "For example let our corpus be as follows:\n",
    "\n",
    ">This is the first document. This document is the second document. This is the third one. Is this the first document?\n",
    "\n",
    "Now assume that we have removed all the punctuations from the documents and also we have converted all the words to lower case. This is what pre-processed documents would look like:\n",
    "\n",
    "````{card}\n",
    "Document-1 (Sentence-1): this is the first document\n",
    "\n",
    "Document-2 (Sentence-2): this document is the second document\n",
    "\n",
    "Document-3 (Sentence-3): this is the third one\n",
    "\n",
    "Document-4 (Sentence-4): is this the first document\n",
    "````\n",
    "\n",
    "The below matrix represents a co-occurrence matrix whose values denote the count of each pair of words occurring together in the given example corpus.\n",
    "\n",
    "![](images/co_occurence.png)\n",
    "\n",
    "The entry '4' in the following table, means that we had 4 sentences in our text where `is` was surrounded by `the`.\n",
    "\n",
    "|          |   document |   first |   is |   one |   second |   the |   third |   this |\n",
    "|:---------|-----------:|--------:|-----:|------:|---------:|------:|--------:|-------:|\n",
    "| document |          0 |       2 |    1 |     0 |        1 |     4 |       0 |      1 |\n",
    "| first    |          2 |       0 |    1 |     0 |        0 |     2 |       0 |      1 |\n",
    "| is       |          1 |       1 |    0 |     0 |        1 |     4 |       1 |      4 |\n",
    "| one      |          0 |       0 |    0 |     0 |        0 |     1 |       1 |      0 |\n",
    "| second   |          1 |       0 |    1 |     0 |        0 |     1 |       0 |      0 |\n",
    "| the      |          4 |       2 |    4 |     1 |        1 |     0 |       1 |      3 |\n",
    "| third    |          0 |       0 |    1 |     1 |        0 |     1 |       0 |      0 |\n",
    "| this     |          1 |       1 |    4 |     0 |        0 |     3 |       0 |      0 |\n",
    "\n",
    "Note that the co-occurrence matrix is always symmetric - the entry with the row word 'the' and the column word 'is' will be 4 as well (as these words co-occur in the very same sentences).\n",
    "\n",
    "Finally, after computing the probability of occurrence for the word `\"first\"` given `\"document\"`, $P(\\text{first} | \\text{document})$ and `\"first\"` given `\"third\"`, $P(\\text{first} | \\text{third})$, it turns out that the most relevant word to `\"first\"` is `\"document\"` as compared to `\"third\"`.\n",
    "\n",
    "### Python Code for GloVe\n",
    "\n",
    "**GloVe** has pre-defined word vectors for around every 6 billion words of English literature along with many other general use characters like comma, braces, and semicolons. This is created by Stanford University. \n",
    "\n",
    "There are 4 varieties available in GloVe:\n",
    "\n",
    "> Four varieties are: *50d, 100d, 200d and 300d*. \n",
    "\n",
    "Here `d` stands for **dimension**. *100d* means, in this file each word has an equivalent vector of size 100. Glove files are simple text files in the form of a dictionary. **Words are key** and **word vectors are values of key**.\n",
    "\n",
    "#### Import essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpjU7ipy2eLm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkBz2-RSSnpI"
   },
   "source": [
    "#### Download the pretrained GloVe data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwo5rBfgShol",
    "outputId": "2bc1e141-e189-438f-a4b8-598543e6efe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-11 14:45:00--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2023-01-11 14:45:01--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2023-01-11 14:45:01--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
      "\n",
      "2023-01-11 14:47:40 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQnatAudTQCO"
   },
   "source": [
    "#### Unzipping the zipped folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FC5p_xP3Spx5",
    "outputId": "dc1eaf76-9566-46b3-b966-c0a46fea3c90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "!unzip glove*.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMTFqzQIUJI7"
   },
   "source": [
    "#### Initialising a tokenizer and fitting it on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4uPwBDSUKa2"
   },
   "outputs": [],
   "source": [
    "train = ['this is the first document',\n",
    "         'this document is the second document',\n",
    "         'this is the third one',\n",
    "         'is this the first document']\n",
    "\n",
    "vocab_size = 8\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2Q9G58hUuAy"
   },
   "source": [
    "#### Opening GloVe file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ts2tWR9aUbUG"
   },
   "outputs": [],
   "source": [
    "glove_file = open('glove.6B.50d.txt', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oyf9fddJUloG"
   },
   "source": [
    "#### Creating a dictionary to store the embeddings\n",
    "\n",
    "Filling the dictionary of embeddings by reading data from the GloVe file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfvnLN6sUhuM"
   },
   "outputs": [],
   "source": [
    "embeddings_dictionary = {}\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfTyxdeZVE41"
   },
   "source": [
    "#### Glove embeddings on our custom training dataset\n",
    "\n",
    "Parsing through all the words in the training dataset and fetching their corresponding vectors from the dictionary and storing them in a matrix. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8LSH1heBWMtt",
    "outputId": "14e6e871-4f6a-497e-fe95-f864902e3de8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('this', 1), ('is', 2), ('the', 3), ('document', 4), ('first', 5), ('second', 6), ('third', 7), ('one', 8)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gfwma7WaU-Y6"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index-1] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKiTfz8PVw6B"
   },
   "source": [
    "#### Displaying embedding matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Og0UTjbFV0yQ",
    "outputId": "f18ef119-3f3a-4101-c107-fa00c6ced9e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6ILF4wKWIMn"
   },
   "source": [
    "We see that each eight words in the vocabulary $V$ has a word embedding of size 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Da6j3HeKaKsk"
   },
   "source": [
    "#### GloVe embeddings using gensim\n",
    "\n",
    "The GLoVe format is slightly different – missing a 1st-line declaration of vector-count & dimensions – than the format that `load_word2vec_format()` supports in gensim.\n",
    "\n",
    "You can download the **GloVe pre-trained word vectors** (like the one we did) and load them easily with gensim.\n",
    "\n",
    "The first step is to convert the GloVe file format to the word2vec file format. The only difference is the addition of a small header line. This can be done by calling the glove2word2vec() function. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBArQPHLaNoY",
    "outputId": "45498a25-49fc-42be-c898-cdeb2ac3ed0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# input glove file\n",
    "glove_input_file = 'glove.6B.100d.txt'\n",
    "\n",
    "# output file we want\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoqLCgxKbBYh"
   },
   "source": [
    "Now we can load it and perform the same `king – man + woman = queen` test as in the [Word2Vec](https://pythonandml.github.io/dlbook/content/word_embeddings/word2vec.html#analogy) (link to previous chapter). The complete code listing is provided below. Note that the converted file is ASCII format, not binary, so we set binary=False when loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ww2ZxDE_a7SE",
    "outputId": "25aca893-bb5b-4f0a-85f7-5429ce60260c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7698541283607483)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load the Stanford GloVe model\n",
    "filename = 'glove.6B.100d.txt.word2vec'\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9POLLHSgdTIH"
   },
   "source": [
    "Running the example prints the same result of `queen`.\n",
    "\n",
    "[Code source for the Gensim part](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/)\n",
    "\n",
    "[Code source for Glove embeddings on our custom training dataset](https://studymachinelearning.com/word-embedding/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}